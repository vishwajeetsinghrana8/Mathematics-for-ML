{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ae7235-ab5b-4e5b-a077-5f38ccf4c416",
   "metadata": {},
   "source": [
    "## Section-2: Tensor Operations Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db447ce9-a713-4f16-b325-fac25f380f39",
   "metadata": {},
   "source": [
    "### **Tensor Transposition**\n",
    "\n",
    "**Tensor Transposition** is the process of rearranging (or permuting) the axes/dimensions of a tensor.\n",
    "\n",
    "It’s the higher-dimensional generalization of **matrix transposition**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Matrix Transposition (2D case)**\n",
    "\n",
    "* A **matrix** is a 2D tensor with shape `(rows, columns)`.\n",
    "* Transposing swaps its two dimensions:\n",
    "\n",
    "  $$\n",
    "  A^T_{ij} = A_{ji}\n",
    "  $$\n",
    "\n",
    "  If $A$ has shape $(m, n)$, then $A^T$ has shape $(n, m)$.\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6\n",
    "\\end{bmatrix}^T\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 4 \\\\\n",
    "2 & 5 \\\\\n",
    "3 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Tensor Transposition (n-D case)**\n",
    "\n",
    "* A **tensor** can have more than 2 dimensions, e.g., 3D: `(depth, height, width)` or `(channels, height, width)`.\n",
    "* **Tensor transposition** means **reordering the dimensions** in a specified way.\n",
    "* In notation, this is often called **permutation of axes**.\n",
    "\n",
    "If $T$ has shape `(D₁, D₂, D₃)`, then:\n",
    "\n",
    "* A transpose with order `(1, 0, 2)` swaps the first two axes.\n",
    "* A transpose with order `(2, 0, 1)` moves the last axis to the front.\n",
    "\n",
    "Mathematically, for a tensor $A$ with components $A_{i_1, i_2, \\dots, i_n}$:\n",
    "\n",
    "$$\n",
    "\\text{transpose}(A, \\text{perm})_{j_1, j_2, \\dots, j_n} = A_{i_{\\text{perm}(1)}, i_{\\text{perm}(2)}, \\dots, i_{\\text{perm}(n)}}\n",
    "$$\n",
    "\n",
    "where `perm` is the permutation order.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Example**\n",
    "\n",
    "Suppose $T$ has shape `(2, 3, 4)`:\n",
    "\n",
    "* Original axes: `(0, 1, 2)` → shape `(2, 3, 4)`\n",
    "* After `transpose(1, 0, 2)` → shape `(3, 2, 4)`\n",
    "* After `transpose(2, 1, 0)` → shape `(4, 3, 2)`\n",
    "\n",
    "In **NumPy**:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "T = np.random.randint(0, 10, (2, 3, 4))\n",
    "T1 = np.transpose(T, (1, 0, 2))  # swap axes 0 and 1\n",
    "print(T.shape)   # (2, 3, 4)\n",
    "print(T1.shape)  # (3, 2, 4)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Why Tensor Transposition is Important**\n",
    "\n",
    "* **Deep Learning**: Changing data layout for operations (e.g., `channels_first` ↔ `channels_last` in CNNs).\n",
    "* **Linear Algebra**: Converting between row-major and column-major forms.\n",
    "* **Data Manipulation**: Preparing tensors for broadcasting, concatenation, or reshaping.\n",
    "* **Performance Optimization**: Some hardware prefers specific memory layouts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe8fc7-a1bd-4d90-ae15-f9e36d7920bc",
   "metadata": {},
   "source": [
    "### 1. Matrix Transposition (2D Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e288469-9800-41ca-8e46-60724a329655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2aac91-5b59-4f3b-a72b-24fdc39af46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (NumPy):\n",
      " [[1 2 3]\n",
      " [4 5 6]]\n",
      "Transpose (NumPy):\n",
      " [[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(\"Original (NumPy):\\n\", A)\n",
    "print(\"Transpose (NumPy):\\n\", A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0819cd89-beb1-40b6-8004-6378ca15363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "917678a0-1e3c-4cc7-aeae-954737d11ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (PyTorch):\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Transpose (PyTorch):\n",
      " tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(\"Original (PyTorch):\\n\", B)\n",
    "print(\"Transpose (PyTorch):\\n\", B.T)   # or torch.transpose(B, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f165d-390c-4cbf-a355-e2fbeca52ac0",
   "metadata": {},
   "source": [
    "### 2. Swap Axes in a 3D Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97df27f2-4329-429e-b01a-763e3e3a3bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 9 8 6]\n",
      "  [4 6 3 9]\n",
      "  [9 2 5 5]]\n",
      "\n",
      " [[8 0 2 5]\n",
      "  [4 5 4 7]\n",
      "  [8 4 3 9]]]\n",
      "Shape before: (2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "T = np.random.randint(0, 10, (2, 3, 4))\n",
    "print(T)\n",
    "print(\"Shape before:\", T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f932102d-d16f-4eb7-a691-486772324508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 9 8 6]\n",
      "  [8 0 2 5]]\n",
      "\n",
      " [[4 6 3 9]\n",
      "  [4 5 4 7]]\n",
      "\n",
      " [[9 2 5 5]\n",
      "  [8 4 3 9]]]\n",
      "Shape after (NumPy): (3, 2, 4)\n"
     ]
    }
   ],
   "source": [
    "T1 = np.transpose(T, (1, 0, 2))  # swap axis 0 and 1\n",
    "print(T1)\n",
    "print(\"Shape after (NumPy):\", T1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d5b0325-8ab0-4d20-af41-85baa5306f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 2, 6, 7],\n",
      "         [1, 4, 6, 8],\n",
      "         [6, 1, 8, 0]],\n",
      "\n",
      "        [[1, 9, 9, 3],\n",
      "         [6, 3, 5, 0],\n",
      "         [0, 9, 3, 6]]])\n",
      "Shape before: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "T_torch = torch.randint(0, 10, (2, 3, 4))\n",
    "print(T_torch)\n",
    "print(\"Shape before:\", T_torch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06d2b5ac-2b35-43f8-a4d2-f87b4f8fad8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 2, 6, 7],\n",
      "         [1, 9, 9, 3]],\n",
      "\n",
      "        [[1, 4, 6, 8],\n",
      "         [6, 3, 5, 0]],\n",
      "\n",
      "        [[6, 1, 8, 0],\n",
      "         [0, 9, 3, 6]]])\n",
      "Shape after (PyTorch): torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "T1_torch = T_torch.permute(1, 0, 2)  # same operation\n",
    "print(T1_torch)\n",
    "print(\"Shape after (PyTorch):\", T1_torch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc6315-fa1b-461b-8086-462412c67a98",
   "metadata": {},
   "source": [
    "### 3. Moving Last Axis to First (Channel Conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb3d5bc-e5f9-4f9b-9571-13f9a6190c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy shape: (32, 32, 3) -> (3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "img = np.random.rand(32, 32, 3)   # HWC (Height, Width, Channels)\n",
    "chw_img = np.transpose(img, (2, 0, 1))  # to CHW\n",
    "print(\"NumPy shape:\", img.shape, \"->\", chw_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "becf824f-04c5-4e02-b2fd-ebec0aaef0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch shape: torch.Size([32, 32, 3]) -> torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "img_torch = torch.rand(32, 32, 3)  # HWC\n",
    "chw_img_torch = img_torch.permute(2, 0, 1)  # to CHW\n",
    "print(\"PyTorch shape:\", img_torch.shape, \"->\", chw_img_torch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eea1d2-ebd6-41db-ae5e-db97528abd2b",
   "metadata": {},
   "source": [
    "### 4. Batch of Matrices Transpose (4D Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de901aca-0080-45b4-bcd2-e62cc154194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy shape: (10, 28, 28, 1) -> (10, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "batch = np.random.rand(10, 28, 28, 1)  # (batch, H, W, C)\n",
    "batch_T = np.transpose(batch, (0, 3, 1, 2))  # (batch, C, H, W)\n",
    "print(\"NumPy shape:\", batch.shape, \"->\", batch_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a011402-1a99-495f-88ef-9f071ec8d5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch shape: torch.Size([10, 28, 28, 1]) -> torch.Size([10, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "batch_torch = torch.rand(10, 28, 28, 1)\n",
    "batch_T_torch = batch_torch.permute(0, 3, 1, 2)\n",
    "print(\"PyTorch shape:\", batch_torch.shape, \"->\", batch_T_torch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7049c4-8bfa-46fb-b8eb-294b5c0a5443",
   "metadata": {},
   "source": [
    "### 5. Swapping Two Specific Dimensions in a Higher Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af32d25e-c7d9-4ed4-ac12-dab327a0d1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy shape: (4, 5, 6, 7) -> (4, 6, 5, 7)\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "X = np.random.rand(4, 5, 6, 7)\n",
    "X_swap = np.swapaxes(X, 1, 2)  # swap axis-1 and axis-2\n",
    "print(\"NumPy shape:\", X.shape, \"->\", X_swap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edd81743-0244-497a-b7db-bdf18aead26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch shape: torch.Size([4, 5, 6, 7]) -> torch.Size([4, 6, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "X_torch = torch.rand(4, 5, 6, 7)\n",
    "X_swap_torch = torch.transpose(X_torch, 1, 2)  # swap dim-1 and dim-2\n",
    "print(\"PyTorch shape:\", X_torch.shape, \"->\", X_swap_torch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8c8048-c3dc-4ee2-a9f8-ef4c474786f6",
   "metadata": {},
   "source": [
    "# **Introduction to Basic Tensor Arithmetical Properties**\n",
    "\n",
    "Tensors are the fundamental data structures used in modern scientific computing, machine learning, and deep learning. A tensor can be thought of as a generalized form of scalars, vectors, and matrices that can extend to any number of dimensions. Since they generalize familiar mathematical objects, tensors naturally inherit a rich set of **arithmetical properties**.\n",
    "\n",
    "Just as numbers, vectors, and matrices follow certain arithmetic rules—such as commutativity of addition, distributivity of multiplication, and the existence of identity elements—tensors also obey these properties. Understanding these rules is important because they guarantee consistency when performing operations like tensor addition, scalar multiplication, elementwise multiplication, and transposition. These properties form the **building blocks** for more advanced concepts in linear algebra, numerical computation, and deep learning frameworks like NumPy and PyTorch.\n",
    "\n",
    "At the most basic level, tensor arithmetic is performed **elementwise** (for addition and multiplication) or **structurally** (for operations like transpose). For example, when two tensors of the same shape are added, the operation applies entry by entry. Similarly, transposing a tensor rearranges its axes, but still preserves key algebraic properties such as $(A + B)^T = A^T + B^T$.\n",
    "\n",
    "By studying these properties, we gain a solid foundation that allows us to:\n",
    "\n",
    "* Manipulate tensors confidently in code (NumPy, PyTorch).\n",
    "* Simplify mathematical expressions in linear algebra.\n",
    "* Ensure correctness when designing deep learning models.\n",
    "* Build intuition for higher-level operations like matrix multiplication, broadcasting, and tensor contractions.\n",
    "\n",
    "In the following sections, we will explore the most important **basic tensor arithmetical properties**—including addition, scalar multiplication, elementwise multiplication, and transpose rules—using simple **2D tensors (matrices)** in NumPy and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79057ccb-7771-4e16-a155-a7802ca2fa2c",
   "metadata": {},
   "source": [
    "## ✅ **Agenda:**\n",
    "\n",
    "1. Commutativity of addition\n",
    "2. Associativity of addition\n",
    "3. Additive identity (zero tensor)\n",
    "4. Additive inverse\n",
    "5. Scalar identity (1)\n",
    "6. Scalar zero (0)\n",
    "7. Associativity of scalar multiplication\n",
    "8. Distributivity of scalar multiplication over addition\n",
    "9. Commutativity of elementwise multiplication\n",
    "10. Distributivity of elementwise multiplication\n",
    "11. Transpose of transpose\n",
    "12. Transpose of sum\n",
    "13. Transpose of scalar multiplication\n",
    "14. Transpose of elementwise product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898baf5-b4f7-4f7a-a554-c284624f62f3",
   "metadata": {},
   "source": [
    "# 📘 **Basic Tensor Arithmetical Properties**\n",
    "\n",
    "Let’s start with **example tensors**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f726c4a0-d780-4e52-9988-5bd94ab93cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy\n",
    "import numpy as np\n",
    "A = np.arange(1, 13).reshape(3, 4)  # 3x4 tensor\n",
    "B = np.ones((3, 4), dtype=int) * 2\n",
    "C = np.full((3, 4), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcf51607-4489-4460-bb20-9aebda2eec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "A_t = torch.arange(1, 13).reshape(3, 4)\n",
    "B_t = torch.ones((3, 4), dtype=torch.int32) * 2\n",
    "C_t = torch.full((3, 4), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ed7ee0c-f7f5-4158-ac3d-7d2fca2d3462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "[[2 2 2 2]\n",
      " [2 2 2 2]\n",
      " [2 2 2 2]]\n",
      "[[3 3 3 3]\n",
      " [3 3 3 3]\n",
      " [3 3 3 3]]\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "tensor([[2, 2, 2, 2],\n",
      "        [2, 2, 2, 2],\n",
      "        [2, 2, 2, 2]], dtype=torch.int32)\n",
      "tensor([[3, 3, 3, 3],\n",
      "        [3, 3, 3, 3],\n",
      "        [3, 3, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "print(B)\n",
    "print(C)\n",
    "\n",
    "print(A_t)\n",
    "print(B_t)\n",
    "print(C_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc0152-9599-49e8-9d13-2ca0cd33372a",
   "metadata": {},
   "source": [
    "So we have:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "9 & 10 & 11 & 12\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "2 & 2 & 2 & 2 \\\\\n",
    "2 & 2 & 2 & 2 \\\\\n",
    "2 & 2 & 2 & 2\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "C =\n",
    "\\begin{bmatrix}\n",
    "3 & 3 & 3 & 3 \\\\\n",
    "3 & 3 & 3 & 3 \\\\\n",
    "3 & 3 & 3 & 3\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1afe1-4bf3-42f2-98e5-991add79515b",
   "metadata": {},
   "source": [
    "## **1. Addition is Commutative**\n",
    "\n",
    "$$\n",
    "A + B = B + A\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4564654f-a2ab-4578-88ff-ca1d2dfc263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 3  4  5  6]\n",
      " [ 7  8  9 10]\n",
      " [11 12 13 14]] \n",
      " [[ 3  4  5  6]\n",
      " [ 7  8  9 10]\n",
      " [11 12 13 14]]\n",
      "PyTorch:\n",
      " tensor([[ 3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10],\n",
      "        [11, 12, 13, 14]]) \n",
      " tensor([[ 3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10],\n",
      "        [11, 12, 13, 14]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", A + B, \"\\n\", B + A)\n",
    "print(\"PyTorch:\\n\", A_t + B_t, \"\\n\", B_t + A_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc787c6-bad9-44a3-92f6-5b131a58eaaa",
   "metadata": {},
   "source": [
    "## **2. Addition is Associative**\n",
    "\n",
    "$$\n",
    "(A + B) + C = A + (B + C)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c8e9864-1c2d-4a89-9f88-b4fa62285210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 6  7  8  9]\n",
      " [10 11 12 13]\n",
      " [14 15 16 17]] \n",
      " [[ 6  7  8  9]\n",
      " [10 11 12 13]\n",
      " [14 15 16 17]]\n",
      "PyTorch:\n",
      " tensor([[ 6,  7,  8,  9],\n",
      "        [10, 11, 12, 13],\n",
      "        [14, 15, 16, 17]]) \n",
      " tensor([[ 6,  7,  8,  9],\n",
      "        [10, 11, 12, 13],\n",
      "        [14, 15, 16, 17]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", (A + B) + C, \"\\n\", A + (B + C))\n",
    "print(\"PyTorch:\\n\", (A_t + B_t) + C_t, \"\\n\", A_t + (B_t + C_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ecc76-495f-40b7-84a9-b6c394278657",
   "metadata": {},
   "source": [
    "## **3. Zero Element (Additive Identity)**\n",
    "\n",
    "$$\n",
    "A + 0 = A\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d155aaf6-f131-4c5d-a99d-e2455930e1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "PyTorch:\n",
      " tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "Z = np.zeros((3, 4), dtype=int)\n",
    "print(\"NumPy:\\n\", A + Z)\n",
    "\n",
    "Z_t = torch.zeros((3, 4), dtype=torch.int32)\n",
    "print(\"PyTorch:\\n\", A_t + Z_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351e133-5f86-4a30-ab61-eb8633af8230",
   "metadata": {},
   "source": [
    "## **4. Additive Inverse**\n",
    "\n",
    "$$\n",
    "A + (-A) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e80a5001-4670-49d7-93a5-78a88c79ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "PyTorch:\n",
      " tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", A + (-A))\n",
    "print(\"PyTorch:\\n\", A_t + (-A_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46276e69-f877-42da-99c7-08da6a1e6f94",
   "metadata": {},
   "source": [
    "## **5. Scalar Multiplication Identity**\n",
    "\n",
    "$$\n",
    "1 \\cdot A = A\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad35f36c-3f35-46ac-b9e5-755aa1d98ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "PyTorch:\n",
      " tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", 1 * A)\n",
    "print(\"PyTorch:\\n\", 1 * A_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197266e8-dede-4e5a-a5ea-1a6ad3ce0c4f",
   "metadata": {},
   "source": [
    "## **6. Multiplication by Zero**\n",
    "\n",
    "$$\n",
    "0 \\cdot A = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a207849-c09d-43f9-b978-046fbe0d1f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "PyTorch:\n",
      " tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", 0 * A)\n",
    "print(\"PyTorch:\\n\", 0 * A_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68abf0ff-ea5a-4438-a820-47ddcfcf58c9",
   "metadata": {},
   "source": [
    "## **7. Scalar Multiplication is Associative**\n",
    "\n",
    "$$\n",
    "(\\alpha \\beta) A = \\alpha (\\beta A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f759e116-23cb-43e1-9487-4041c3c75fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 6 12 18 24]\n",
      " [30 36 42 48]\n",
      " [54 60 66 72]] \n",
      " [[ 6 12 18 24]\n",
      " [30 36 42 48]\n",
      " [54 60 66 72]]\n",
      "PyTorch:\n",
      " tensor([[ 6, 12, 18, 24],\n",
      "        [30, 36, 42, 48],\n",
      "        [54, 60, 66, 72]]) \n",
      " tensor([[ 6, 12, 18, 24],\n",
      "        [30, 36, 42, 48],\n",
      "        [54, 60, 66, 72]])\n"
     ]
    }
   ],
   "source": [
    "alpha, beta = 2, 3\n",
    "print(\"NumPy:\\n\", (alpha * beta) * A, \"\\n\", alpha * (beta * A))\n",
    "print(\"PyTorch:\\n\", (alpha * beta) * A_t, \"\\n\", alpha * (beta * A_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7216d7e9-2098-4b25-9e0b-f030c796de81",
   "metadata": {},
   "source": [
    "## **8. Scalar Multiplication Distributes Over Addition**\n",
    "\n",
    "$$\n",
    "\\alpha (A + B) = \\alpha A + \\alpha B\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2be02e3-9f39-41e1-9048-919a32489313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[15 20 25 30]\n",
      " [35 40 45 50]\n",
      " [55 60 65 70]] \n",
      " [[15 20 25 30]\n",
      " [35 40 45 50]\n",
      " [55 60 65 70]]\n",
      "PyTorch:\n",
      " tensor([[15, 20, 25, 30],\n",
      "        [35, 40, 45, 50],\n",
      "        [55, 60, 65, 70]]) \n",
      " tensor([[15, 20, 25, 30],\n",
      "        [35, 40, 45, 50],\n",
      "        [55, 60, 65, 70]])\n"
     ]
    }
   ],
   "source": [
    "alpha = 5\n",
    "print(\"NumPy:\\n\", alpha * (A + B), \"\\n\", alpha * A + alpha * B)\n",
    "print(\"PyTorch:\\n\", alpha * (A_t + B_t), \"\\n\", alpha * A_t + alpha * B_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb5ca8-9672-4f55-a16a-ba2251fd64b8",
   "metadata": {},
   "source": [
    "## **9. Elementwise Multiplication is Commutative**\n",
    "\n",
    "$$\n",
    "A \\odot B = B \\odot A\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d0ed347-2325-4ab1-9e28-1e2c6c94fd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 2  4  6  8]\n",
      " [10 12 14 16]\n",
      " [18 20 22 24]] \n",
      " [[ 2  4  6  8]\n",
      " [10 12 14 16]\n",
      " [18 20 22 24]]\n",
      "PyTorch:\n",
      " tensor([[ 2,  4,  6,  8],\n",
      "        [10, 12, 14, 16],\n",
      "        [18, 20, 22, 24]]) \n",
      " tensor([[ 2,  4,  6,  8],\n",
      "        [10, 12, 14, 16],\n",
      "        [18, 20, 22, 24]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", A * B, \"\\n\", B * A)\n",
    "print(\"PyTorch:\\n\", A_t * B_t, \"\\n\", B_t * A_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255acd62-7817-40d9-960f-ad6053b07aed",
   "metadata": {},
   "source": [
    "## **10. Elementwise Multiplication Distributes Over Addition**\n",
    "\n",
    "$$\n",
    "A \\odot (B + C) = A \\odot B + A \\odot C\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a08517a1-497e-42c1-a7ee-5a1cc668df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 5 10 15 20]\n",
      " [25 30 35 40]\n",
      " [45 50 55 60]] \n",
      " [[ 5 10 15 20]\n",
      " [25 30 35 40]\n",
      " [45 50 55 60]]\n",
      "PyTorch:\n",
      " tensor([[ 5, 10, 15, 20],\n",
      "        [25, 30, 35, 40],\n",
      "        [45, 50, 55, 60]]) \n",
      " tensor([[ 5, 10, 15, 20],\n",
      "        [25, 30, 35, 40],\n",
      "        [45, 50, 55, 60]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", A * (B + C), \"\\n\", A * B + A * C)\n",
    "print(\"PyTorch:\\n\", A_t * (B_t + C_t), \"\\n\", A_t * B_t + A_t * C_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab643af-786a-4c09-85be-9a1dcedea311",
   "metadata": {},
   "source": [
    "## **11. Transpose of Transpose**\n",
    "\n",
    "$$\n",
    "(A^T)^T = A\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f50594c0-4d06-48d7-9434-2a2227f9638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "PyTorch:\n",
      " tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", (A.T).T)\n",
    "print(\"PyTorch:\\n\", (A_t.T).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e59c8-901c-49c3-ba19-eef033c1b4ca",
   "metadata": {},
   "source": [
    "## **12. Transpose of Sum**\n",
    "\n",
    "$$\n",
    "(A + B)^T = A^T + B^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18f2ce9d-d07b-4422-93cd-2959bd0b0c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 3  7 11]\n",
      " [ 4  8 12]\n",
      " [ 5  9 13]\n",
      " [ 6 10 14]] \n",
      " [[ 3  7 11]\n",
      " [ 4  8 12]\n",
      " [ 5  9 13]\n",
      " [ 6 10 14]]\n",
      "PyTorch:\n",
      " tensor([[ 3,  7, 11],\n",
      "        [ 4,  8, 12],\n",
      "        [ 5,  9, 13],\n",
      "        [ 6, 10, 14]]) \n",
      " tensor([[ 3,  7, 11],\n",
      "        [ 4,  8, 12],\n",
      "        [ 5,  9, 13],\n",
      "        [ 6, 10, 14]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", (A + B).T, \"\\n\", A.T + B.T)\n",
    "print(\"PyTorch:\\n\", (A_t + B_t).T, \"\\n\", A_t.T + B_t.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb347c0-19b8-4e05-b915-065cb3eb1529",
   "metadata": {},
   "source": [
    "## **13. Transpose of Scalar Multiple**\n",
    "\n",
    "$$\n",
    "(\\alpha A)^T = \\alpha A^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1217f3d1-1b62-46bc-b0c9-f314348b1614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 4 20 36]\n",
      " [ 8 24 40]\n",
      " [12 28 44]\n",
      " [16 32 48]] \n",
      " [[ 4 20 36]\n",
      " [ 8 24 40]\n",
      " [12 28 44]\n",
      " [16 32 48]]\n",
      "PyTorch:\n",
      " tensor([[ 4, 20, 36],\n",
      "        [ 8, 24, 40],\n",
      "        [12, 28, 44],\n",
      "        [16, 32, 48]]) \n",
      " tensor([[ 4, 20, 36],\n",
      "        [ 8, 24, 40],\n",
      "        [12, 28, 44],\n",
      "        [16, 32, 48]])\n"
     ]
    }
   ],
   "source": [
    "alpha = 4\n",
    "print(\"NumPy:\\n\", (alpha * A).T, \"\\n\", alpha * (A.T))\n",
    "print(\"PyTorch:\\n\", (alpha * A_t).T, \"\\n\", alpha * (A_t.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b8798a-0e93-450b-acbe-dcbdfd1c340e",
   "metadata": {},
   "source": [
    "## **14. Transpose of Elementwise Product**\n",
    "\n",
    "$$\n",
    "(A \\odot B)^T = A^T \\odot B^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2e99f79-93fb-47ae-8663-d8df55b63016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 2 10 18]\n",
      " [ 4 12 20]\n",
      " [ 6 14 22]\n",
      " [ 8 16 24]] \n",
      " [[ 2 10 18]\n",
      " [ 4 12 20]\n",
      " [ 6 14 22]\n",
      " [ 8 16 24]]\n",
      "PyTorch:\n",
      " tensor([[ 2, 10, 18],\n",
      "        [ 4, 12, 20],\n",
      "        [ 6, 14, 22],\n",
      "        [ 8, 16, 24]]) \n",
      " tensor([[ 2, 10, 18],\n",
      "        [ 4, 12, 20],\n",
      "        [ 6, 14, 22],\n",
      "        [ 8, 16, 24]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", (A * B).T, \"\\n\", A.T * B.T)\n",
    "print(\"PyTorch:\\n\", (A_t * B_t).T, \"\\n\", A_t.T * B_t.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee63be3-1e8e-486a-a641-149ab28f21fa",
   "metadata": {},
   "source": [
    "# **What is Tensor Reduction?**\n",
    "\n",
    "**Tensor Reduction** refers to the process of **reducing a tensor along one or more dimensions (axes)** by applying an operation such as **sum, mean, max, min, product, etc.**\n",
    "\n",
    "Instead of keeping the full tensor, we “reduce” its dimensionality by aggregating values.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Intuition**\n",
    "\n",
    "* Suppose we have a **2D tensor (matrix)**:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* **Full reduction (no axis specified):**\n",
    "  Sum of all elements → $1+2+\\dots+9 = 45$\n",
    "  Mean of all elements → $\\frac{45}{9} = 5$\n",
    "\n",
    "* **Reduction along axis 0 (rows):**\n",
    "  Collapse rows → column-wise operation\n",
    "\n",
    "  $$\n",
    "  \\text{sum}(A, \\text{axis}=0) = [12, 15, 18]\n",
    "  $$\n",
    "\n",
    "* **Reduction along axis 1 (columns):**\n",
    "  Collapse columns → row-wise operation\n",
    "\n",
    "  $$\n",
    "  \\text{sum}(A, \\text{axis}=1) = [6, 15, 24]\n",
    "  $$\n",
    "\n",
    "👉 In short: **Reduction shrinks dimensions by aggregating values.**\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Common Reduction Operations**\n",
    "\n",
    "### Reduction Operations You Can Perform:\n",
    "\n",
    "1. **Sum** (`sum`, `reduce_sum`)\n",
    "2. **Mean** (`mean`, `reduce_mean`)\n",
    "3. **Max / Min** (`max`, `reduce_max`, `min`, `reduce_min`)\n",
    "4. **Product** (`prod`, `reduce_prod`)\n",
    "5. **Argmax / Argmin** (indices of extrema)\n",
    "6. **Std / Variance** (`std`, `var`, `reduce_std`, `reduce_variance`)\n",
    "7. **Any / All** (logical reductions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d8102d-5a35-4fd9-bb8a-2dae2e6c223b",
   "metadata": {},
   "source": [
    "## 📘 **Example Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcbcff49-3d3c-47a6-b36e-e0b148f15bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ce5b0f9-65f1-46d9-a676-6eeffe4f100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy\n",
    "A_np = np.array([[1, 2, 3],\n",
    "                 [4, 5, 6],\n",
    "                 [7, 8, 9]])\n",
    "\n",
    "# PyTorch\n",
    "A_torch = torch.tensor([[1, 2, 3],\n",
    "                        [4, 5, 6],\n",
    "                        [7, 8, 9]])\n",
    "\n",
    "# TensorFlow\n",
    "A_tf = tf.constant([[1, 2, 3],\n",
    "                    [4, 5, 6],\n",
    "                    [7, 8, 9]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747d36c-010b-4332-8e28-44ea87275de7",
   "metadata": {},
   "source": [
    "## **1. Sum Reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22095379-6457-423b-bbd1-1a803080143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Sum (all): 45\n",
      "NumPy Sum axis=0: [12 15 18]\n",
      "NumPy Sum axis=1: [ 6 15 24]\n",
      "PyTorch Sum (all): tensor(45)\n",
      "PyTorch Sum dim=0: tensor([12, 15, 18])\n",
      "PyTorch Sum dim=1: tensor([ 6, 15, 24])\n",
      "TF Sum (all): 45\n",
      "TF Sum axis=0: [12 15 18]\n",
      "TF Sum axis=1: [ 6 15 24]\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Sum (all):\", np.sum(A_np))  \n",
    "print(\"NumPy Sum axis=0:\", np.sum(A_np, axis=0))  \n",
    "print(\"NumPy Sum axis=1:\", np.sum(A_np, axis=1))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Sum (all):\", torch.sum(A_torch))  \n",
    "print(\"PyTorch Sum dim=0:\", torch.sum(A_torch, dim=0))  \n",
    "print(\"PyTorch Sum dim=1:\", torch.sum(A_torch, dim=1))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Sum (all):\", tf.reduce_sum(A_tf).numpy())  \n",
    "print(\"TF Sum axis=0:\", tf.reduce_sum(A_tf, axis=0).numpy())  \n",
    "print(\"TF Sum axis=1:\", tf.reduce_sum(A_tf, axis=1).numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b858979-b17a-4732-9fb0-4114ae0d4957",
   "metadata": {},
   "source": [
    "## **2. Mean Reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3e82506-f7d1-4c52-ad9f-604f4774d6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Mean (all): 5.0\n",
      "NumPy Mean axis=0: [4. 5. 6.]\n",
      "NumPy Mean axis=1: [2. 5. 8.]\n",
      "PyTorch Mean (all): tensor(5.)\n",
      "PyTorch Mean dim=0: tensor([4., 5., 6.])\n",
      "PyTorch Mean dim=1: tensor([2., 5., 8.])\n",
      "TF Mean (all): 5\n",
      "TF Mean axis=0: [4 5 6]\n",
      "TF Mean axis=1: [2 5 8]\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Mean (all):\", np.mean(A_np))  \n",
    "print(\"NumPy Mean axis=0:\", np.mean(A_np, axis=0))  \n",
    "print(\"NumPy Mean axis=1:\", np.mean(A_np, axis=1))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Mean (all):\", torch.mean(A_torch.float()))  \n",
    "print(\"PyTorch Mean dim=0:\", torch.mean(A_torch.float(), dim=0))  \n",
    "print(\"PyTorch Mean dim=1:\", torch.mean(A_torch.float(), dim=1))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Mean (all):\", tf.reduce_mean(A_tf).numpy())  \n",
    "print(\"TF Mean axis=0:\", tf.reduce_mean(A_tf, axis=0).numpy())  \n",
    "print(\"TF Mean axis=1:\", tf.reduce_mean(A_tf, axis=1).numpy())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e4bb10-f600-4358-9b98-06120365acd5",
   "metadata": {},
   "source": [
    "## **3. Max Reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3764802a-84df-42f9-8abd-fcc6345320d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Max (all): 9\n",
      "NumPy Max axis=0: [7 8 9]\n",
      "NumPy Max axis=1: [3 6 9]\n",
      "PyTorch Max (all): tensor(9)\n",
      "PyTorch Max dim=0: torch.return_types.max(\n",
      "values=tensor([7, 8, 9]),\n",
      "indices=tensor([2, 2, 2]))\n",
      "PyTorch Max dim=1: torch.return_types.max(\n",
      "values=tensor([3, 6, 9]),\n",
      "indices=tensor([2, 2, 2]))\n",
      "TF Max (all): 9\n",
      "TF Max axis=0: [7 8 9]\n",
      "TF Max axis=1: [3 6 9]\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Max (all):\", np.max(A_np))  \n",
    "print(\"NumPy Max axis=0:\", np.max(A_np, axis=0))  \n",
    "print(\"NumPy Max axis=1:\", np.max(A_np, axis=1))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Max (all):\", torch.max(A_torch))  \n",
    "print(\"PyTorch Max dim=0:\", torch.max(A_torch, dim=0))  \n",
    "print(\"PyTorch Max dim=1:\", torch.max(A_torch, dim=1))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Max (all):\", tf.reduce_max(A_tf).numpy())  \n",
    "print(\"TF Max axis=0:\", tf.reduce_max(A_tf, axis=0).numpy())  \n",
    "print(\"TF Max axis=1:\", tf.reduce_max(A_tf, axis=1).numpy())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608524c3-9ecb-43fa-aa1d-830f455857a3",
   "metadata": {},
   "source": [
    "## **4. Min Reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b380018-95ee-4f54-aaea-70ef03cf4e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Min (all): 1\n",
      "NumPy Min axis=0: [1 2 3]\n",
      "NumPy Min axis=1: [1 4 7]\n",
      "PyTorch Min (all): tensor(1)\n",
      "PyTorch Min dim=0: torch.return_types.min(\n",
      "values=tensor([1, 2, 3]),\n",
      "indices=tensor([0, 0, 0]))\n",
      "PyTorch Min dim=1: torch.return_types.min(\n",
      "values=tensor([1, 4, 7]),\n",
      "indices=tensor([0, 0, 0]))\n",
      "TF Min (all): 1\n",
      "TF Min axis=0: [1 2 3]\n",
      "TF Min axis=1: [1 4 7]\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Min (all):\", np.min(A_np))  \n",
    "print(\"NumPy Min axis=0:\", np.min(A_np, axis=0))  \n",
    "print(\"NumPy Min axis=1:\", np.min(A_np, axis=1))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Min (all):\", torch.min(A_torch))  \n",
    "print(\"PyTorch Min dim=0:\", torch.min(A_torch, dim=0))  \n",
    "print(\"PyTorch Min dim=1:\", torch.min(A_torch, dim=1))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Min (all):\", tf.reduce_min(A_tf).numpy())  \n",
    "print(\"TF Min axis=0:\", tf.reduce_min(A_tf, axis=0).numpy())  \n",
    "print(\"TF Min axis=1:\", tf.reduce_min(A_tf, axis=1).numpy())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a9f7c-0acc-4f37-ac83-02d5973b8d28",
   "metadata": {},
   "source": [
    "## **5. Product Reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f09502dd-b48a-42b9-9e49-ef114bd5c45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Prod (all): 362880\n",
      "NumPy Prod axis=0: [ 28  80 162]\n",
      "NumPy Prod axis=1: [  6 120 504]\n",
      "PyTorch Prod (all): tensor(362880)\n",
      "PyTorch Prod dim=0: tensor([ 28,  80, 162])\n",
      "PyTorch Prod dim=1: tensor([  6, 120, 504])\n",
      "TF Prod (all): 362880\n",
      "TF Prod axis=0: [ 28  80 162]\n",
      "TF Prod axis=1: [  6 120 504]\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Prod (all):\", np.prod(A_np))  \n",
    "print(\"NumPy Prod axis=0:\", np.prod(A_np, axis=0))  \n",
    "print(\"NumPy Prod axis=1:\", np.prod(A_np, axis=1))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Prod (all):\", torch.prod(A_torch))  \n",
    "print(\"PyTorch Prod dim=0:\", torch.prod(A_torch, dim=0))  \n",
    "print(\"PyTorch Prod dim=1:\", torch.prod(A_torch, dim=1))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Prod (all):\", tf.reduce_prod(A_tf).numpy())  \n",
    "print(\"TF Prod axis=0:\", tf.reduce_prod(A_tf, axis=0).numpy())  \n",
    "print(\"TF Prod axis=1:\", tf.reduce_prod(A_tf, axis=1).numpy())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a483dc-ef5d-4aae-b063-901838357bd7",
   "metadata": {},
   "source": [
    "## **6. Argmax (Index of Max)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd211621-fdde-4d2f-a1d5-50b1918677d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Argmax (all): 8\n",
      "NumPy Argmax axis=0: [2 2 2]\n",
      "NumPy Argmax axis=1: [2 2 2]\n",
      "PyTorch Argmax (all): tensor(8)\n",
      "PyTorch Argmax dim=0: tensor([2, 2, 2])\n",
      "PyTorch Argmax dim=1: tensor([2, 2, 2])\n",
      "TF Argmax axis=0: [2 2 2]\n",
      "TF Argmax axis=1: [2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Argmax (all):\", np.argmax(A_np))  \n",
    "print(\"NumPy Argmax axis=0:\", np.argmax(A_np, axis=0))  \n",
    "print(\"NumPy Argmax axis=1:\", np.argmax(A_np, axis=1))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Argmax (all):\", torch.argmax(A_torch))  \n",
    "print(\"PyTorch Argmax dim=0:\", torch.argmax(A_torch, dim=0))  \n",
    "print(\"PyTorch Argmax dim=1:\", torch.argmax(A_torch, dim=1))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Argmax axis=0:\", tf.argmax(A_tf, axis=0).numpy())  \n",
    "print(\"TF Argmax axis=1:\", tf.argmax(A_tf, axis=1).numpy())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50981160-b944-4200-9ef7-f137bcc7daa8",
   "metadata": {},
   "source": [
    "## **7. Argmin (Index of Min)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "379e0771-755f-4e5d-9d78-80c7cc888fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Argmin (all): 0\n",
      "NumPy Argmin axis=0: [0 0 0]\n",
      "NumPy Argmin axis=1: [0 0 0]\n",
      "PyTorch Argmin (all): tensor(0)\n",
      "PyTorch Argmin dim=0: tensor([0, 0, 0])\n",
      "PyTorch Argmin dim=1: tensor([0, 0, 0])\n",
      "TF Argmin axis=0: [0 0 0]\n",
      "TF Argmin axis=1: [0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Argmin (all):\", np.argmin(A_np))  \n",
    "print(\"NumPy Argmin axis=0:\", np.argmin(A_np, axis=0))  \n",
    "print(\"NumPy Argmin axis=1:\", np.argmin(A_np, axis=1))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Argmin (all):\", torch.argmin(A_torch))  \n",
    "print(\"PyTorch Argmin dim=0:\", torch.argmin(A_torch, dim=0))  \n",
    "print(\"PyTorch Argmin dim=1:\", torch.argmin(A_torch, dim=1))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Argmin axis=0:\", tf.argmin(A_tf, axis=0).numpy())  \n",
    "print(\"TF Argmin axis=1:\", tf.argmin(A_tf, axis=1).numpy())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8d896-b357-4718-ada4-6022b5912637",
   "metadata": {},
   "source": [
    "## **8. Standard Deviation & Variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29f2d447-3361-456d-9fc3-02593e073489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Std (all): 2.581988897471611\n",
      "NumPy Var (all): 6.666666666666667\n",
      "PyTorch Std (all): tensor(2.7386)\n",
      "PyTorch Var (all): tensor(7.5000)\n",
      "TF Std (all): 2.5819888\n",
      "TF Var (all): 6.6666665\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Std (all):\", np.std(A_np))  \n",
    "print(\"NumPy Var (all):\", np.var(A_np))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Std (all):\", torch.std(A_torch.float()))  \n",
    "print(\"PyTorch Var (all):\", torch.var(A_torch.float()))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Std (all):\", tf.math.reduce_std(tf.cast(A_tf, tf.float32)).numpy())  \n",
    "print(\"TF Var (all):\", tf.math.reduce_variance(tf.cast(A_tf, tf.float32)).numpy())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64634882-9919-443c-bb60-818c75222b40",
   "metadata": {},
   "source": [
    "## **9. Logical Reductions (any, all)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51040e9f-0b01-4fa6-8af8-c5ca450acf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Any: True\n",
      "NumPy All: False\n",
      "PyTorch Any: tensor(True)\n",
      "PyTorch All: tensor(True)\n",
      "TF Any: True\n",
      "TF All: True\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Any:\", np.any(A_np > 5))  \n",
    "print(\"NumPy All:\", np.all(A_np < 0))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Any:\", torch.any(A_torch > 5))  \n",
    "print(\"PyTorch All:\", torch.all(A_torch > 0))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Any:\", tf.reduce_any(A_tf > 5).numpy())  \n",
    "print(\"TF All:\", tf.reduce_all(A_tf > 0).numpy())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21c8a5-fdf4-4f15-849e-eb502d1f30b6",
   "metadata": {},
   "source": [
    "# **What are Tensor Attributes?**\n",
    "\n",
    "In **linear algebra** and in frameworks like **NumPy, PyTorch, TensorFlow**, a **tensor** is basically a generalized multi-dimensional array.\n",
    "Every tensor has **properties (metadata)** that describe its structure and nature.\n",
    "\n",
    "These properties are called **Tensor Attributes**.\n",
    "They tell us things like:\n",
    "\n",
    "* 📏 **Shape** → Dimensions of the tensor (rows, cols, etc.)\n",
    "* 🔢 **Rank / Dimension** → Number of axes\n",
    "* 📚 **Size / Numel** → Total number of elements\n",
    "* 🧮 **Data Type (`dtype`)** → What type of numbers (int, float, etc.)\n",
    "* 💻 **Device** → Whether stored in CPU or GPU (PyTorch/TensorFlow)\n",
    "* 🧾 **Requires Grad** → Whether to track gradient (PyTorch only, for autograd)\n",
    "\n",
    "## 📘 **Basic Tensor Attributes (NumPy & PyTorch)**\n",
    "\n",
    "### Example Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4c2e53e-cd9e-480a-ba1f-6d0c7b8459b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy\n",
    "import numpy as np\n",
    "A = np.arange(1, 13).reshape(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db4f6671-e22a-42a9-a671-4b98277980eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "A_t = torch.arange(1, 13).reshape(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8555594-c736-489f-85b4-b9375e994755",
   "metadata": {},
   "source": [
    "So we have:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "9 & 10 & 11 & 12\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71876bd3-b2be-4421-9337-b434d1080ffa",
   "metadata": {},
   "source": [
    "## **1. Shape**\n",
    "\n",
    "* Describes the dimensions of the tensor.\n",
    "* For a **3×4 matrix**, shape = `(3, 4)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99e14de8-4232-4594-aebb-c56a603f2c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Shape: (3, 4)\n",
      "PyTorch Shape: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy Shape:\", A.shape)\n",
    "print(\"PyTorch Shape:\", A_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834d354-2ec0-4556-9fe2-d2b5370c60b5",
   "metadata": {},
   "source": [
    "## **2. Number of Dimensions (Rank)**\n",
    "\n",
    "* Tells how many axes (dimensions) the tensor has.\n",
    "* A matrix has **2 dimensions** (rows, columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84254aac-ce2a-4f7e-87de-6f7cb410b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy ndim: 2\n",
      "PyTorch dim: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy ndim:\", A.ndim)\n",
    "print(\"PyTorch dim:\", A_t.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8029625d-e64f-4eb3-b4f5-37ed83f7cab1",
   "metadata": {},
   "source": [
    "## **3. Size (Total Number of Elements)**\n",
    "\n",
    "* The product of all dimensions.\n",
    "* For `(3,4)` → $3 \\times 4 = 12$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba485dea-abf7-4ec8-8dea-75301e304429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy size: 12\n",
      "PyTorch numel: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy size:\", A.size)\n",
    "print(\"PyTorch numel:\", A_t.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c79db-c9a3-490b-b776-2d3aeaf12163",
   "metadata": {},
   "source": [
    "## **4. Data Type**\n",
    "\n",
    "* Defines the type of elements stored (e.g., int32, float64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27b72ba7-5d62-4292-9a97-216823e60d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy dtype: int64\n",
      "PyTorch dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy dtype:\", A.dtype)\n",
    "print(\"PyTorch dtype:\", A_t.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91602e6-6c3c-4cc5-b39c-47d3a84428e1",
   "metadata": {},
   "source": [
    "## **5. Item Size**\n",
    "\n",
    "* Size in bytes of each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91accc4e-93c6-491a-808f-5a294ee5388c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy itemsize: 8\n",
      "PyTorch element_size: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy itemsize:\", A.itemsize)\n",
    "print(\"PyTorch element_size:\", A_t.element_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965e40c9-7060-4a3e-9f80-a657e982065c",
   "metadata": {},
   "source": [
    "## **6. Total Memory (in Bytes)**\n",
    "\n",
    "* `size × itemsize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b508648-66a2-47be-acc4-18044e9259d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy nbytes: 96\n",
      "PyTorch memory (bytes): 96\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy nbytes:\", A.nbytes)\n",
    "print(\"PyTorch memory (bytes):\", A_t.element_size() * A_t.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd4ba2-f2fb-41e0-96f1-b7f0c05bf163",
   "metadata": {},
   "source": [
    "## **7. Device**\n",
    "\n",
    "* In PyTorch, tensors can be on **CPU** or **GPU**.\n",
    "* NumPy arrays are always on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "41b465d4-35b0-41e2-b72a-8188ebbf0009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy device: CPU (default)\n",
      "PyTorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy device: CPU (default)\")\n",
    "print(\"PyTorch device:\", A_t.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089999d4-207b-48e1-b090-5f5c6df1e66e",
   "metadata": {},
   "source": [
    "## **8. Requires Gradient (Autograd)**\n",
    "\n",
    "* Specific to PyTorch (for deep learning).\n",
    "* Tells if the tensor tracks gradients for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3cd629b-877f-4576-9947-ca2792f76291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch requires_grad:\", A_t.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9239eb7-5c8e-40d9-89df-8c1b7c270b36",
   "metadata": {},
   "source": [
    "## **9. Strides**\n",
    "\n",
    "* Strides describe how many bytes to move in memory to step along each dimension.\n",
    "* Important for memory layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3f814ab-c4a3-4beb-9b34-014aa0d6dc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy strides: (32, 8)\n",
      "PyTorch strides: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy strides:\", A.strides)\n",
    "print(\"PyTorch strides:\", A_t.stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51f5fe-715c-419c-bbf0-79b60130ff73",
   "metadata": {},
   "source": [
    "## **10. Indexing & Slicing Support**\n",
    "\n",
    "* Both NumPy and PyTorch support accessing elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c98d95c-de29-4768-a973-24142426b9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy element A[1,2]: 7\n",
      "PyTorch element A_t[1,2]: tensor(7)\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy element A[1,2]:\", A[1,2])\n",
    "print(\"PyTorch element A_t[1,2]:\", A_t[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1be8c1-b997-453d-aa5a-8d3a0877cd7a",
   "metadata": {},
   "source": [
    "# 🧮 **Some More Examples of Tensor Attributes**\n",
    "\n",
    "Let’s use a **3x4 (2D) Tensor** for clarity.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. NumPy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "07254c55-a0d2-456f-9fbe-b5c38b0e2817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "Shape: (3, 4)\n",
      "Rank (ndim): 2\n",
      "Size (numel): 12\n",
      "Dtype: int64\n",
      "Itemsize (bytes per element): 8\n",
      "Total Memory (bytes): 96\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 3x4 tensor\n",
    "tensor = np.array([[1, 2, 3, 4],\n",
    "                   [5, 6, 7, 8],\n",
    "                   [9, 10, 11, 12]])\n",
    "\n",
    "print(\"Tensor:\\n\", tensor)\n",
    "\n",
    "# Attributes\n",
    "print(\"Shape:\", tensor.shape)        # (3, 4)\n",
    "print(\"Rank (ndim):\", tensor.ndim)   # 2\n",
    "print(\"Size (numel):\", tensor.size)  # 12\n",
    "print(\"Dtype:\", tensor.dtype)        # int64 (default)\n",
    "print(\"Itemsize (bytes per element):\", tensor.itemsize)\n",
    "print(\"Total Memory (bytes):\", tensor.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d2d08-cc54-4646-9068-ad7e7cfa6ba8",
   "metadata": {},
   "source": [
    "## **2. PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "744c5dfb-81ec-46d9-af42-968f9142fc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\n",
      " tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]])\n",
      "Shape: torch.Size([3, 4])\n",
      "Rank (ndim): 2\n",
      "Numel (size): 12\n",
      "Dtype: torch.float32\n",
      "Device: cpu\n",
      "Requires Grad: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 3x4 tensor\n",
    "tensor = torch.tensor([[1, 2, 3, 4],\n",
    "                       [5, 6, 7, 8],\n",
    "                       [9, 10, 11, 12]], dtype=torch.float32)\n",
    "\n",
    "print(\"Tensor:\\n\", tensor)\n",
    "\n",
    "# Attributes\n",
    "print(\"Shape:\", tensor.shape)           # torch.Size([3, 4])\n",
    "print(\"Rank (ndim):\", tensor.ndim)      # 2\n",
    "print(\"Numel (size):\", tensor.numel())  # 12\n",
    "print(\"Dtype:\", tensor.dtype)           # torch.float32\n",
    "print(\"Device:\", tensor.device)         # cpu\n",
    "print(\"Requires Grad:\", tensor.requires_grad)  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237a333d-f044-4df4-ac78-2955cb2abc39",
   "metadata": {},
   "source": [
    "## **3. TensorFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "067f93c5-d0b8-471c-9d2b-e1e8d72d8eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\n",
      " tf.Tensor(\n",
      "[[ 1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.]\n",
      " [ 9. 10. 11. 12.]], shape=(3, 4), dtype=float32)\n",
      "Shape: (3, 4)\n",
      "Rank (ndim): tf.Tensor(2, shape=(), dtype=int32)\n",
      "Size (numel): 12\n",
      "Dtype: <dtype: 'float32'>\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 3x4 tensor\n",
    "tensor = tf.constant([[1, 2, 3, 4],\n",
    "                      [5, 6, 7, 8],\n",
    "                      [9, 10, 11, 12]], dtype=tf.float32)\n",
    "\n",
    "print(\"Tensor:\\n\", tensor)\n",
    "\n",
    "# Attributes\n",
    "print(\"Shape:\", tensor.shape)          # (3, 4)\n",
    "print(\"Rank (ndim):\", tf.rank(tensor)) # 2\n",
    "print(\"Size (numel):\", tf.size(tensor).numpy()) # 12\n",
    "print(\"Dtype:\", tensor.dtype)          # float32\n",
    "print(\"Device:\", tensor.device)        # CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368e4e21-006d-469b-bc81-f01ffdf42a15",
   "metadata": {},
   "source": [
    "# 📘 **Dot Product in Algebra**\n",
    "\n",
    "### 🔹 Definition\n",
    "\n",
    "The **dot product** (also called the **scalar product** or **inner product**) is an algebraic operation that takes two **vectors of the same dimension** and returns a **single scalar (number)**.\n",
    "\n",
    "If we have two vectors:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} = [a_1, a_2, a_3, \\dots, a_n], \\quad \\mathbf{b} = [b_1, b_2, b_3, \\dots, b_n]\n",
    "$$\n",
    "\n",
    "The dot product is:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + a_3 b_3 + \\dots + a_n b_n\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Geometric Interpretation\n",
    "\n",
    "The dot product is also defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\|\\mathbf{a}\\|$ = magnitude (length) of vector **a**\n",
    "* $\\|\\mathbf{b}\\|$ = magnitude of vector **b**\n",
    "* $\\theta$ = angle between the vectors\n",
    "\n",
    "👉 This means the dot product measures **how aligned two vectors are**:\n",
    "\n",
    "* If vectors point in the **same direction**, dot product is positive.\n",
    "* If vectors are **orthogonal (90°)**, dot product is 0.\n",
    "* If vectors point in **opposite directions**, dot product is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ddefbf-7f82-4e7b-b7a9-bdbcc80334a7",
   "metadata": {},
   "source": [
    "# 🧮 **Examples of Dot Product**\n",
    "\n",
    "## **1. Simple 1D Vector Example**\n",
    "\n",
    "$$\n",
    "a = [1, 2, 3], \\quad b = [4, 5, 6]\n",
    "$$\n",
    "\n",
    "$$\n",
    "a \\cdot b = 1\\cdot4 + 2\\cdot5 + 3\\cdot6 = 4 + 10 + 18 = 32\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Matrix-Vector Example**\n",
    "\n",
    "If we treat dot product as **row by column multiplication**:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "b =\n",
    "\\begin{bmatrix}\n",
    "5 \\\\\n",
    "6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A \\cdot b =\n",
    "\\begin{bmatrix}\n",
    "1\\cdot5 + 2\\cdot6 \\\\\n",
    "3\\cdot5 + 4\\cdot6\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "17 \\\\\n",
    "39\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Orthogonal Vectors Example**\n",
    "\n",
    "$$\n",
    "a = [1, 0], \\quad b = [0, 1]\n",
    "$$\n",
    "\n",
    "$$\n",
    "a \\cdot b = 1\\cdot0 + 0\\cdot1 = 0\n",
    "$$\n",
    "\n",
    "These are **perpendicular vectors**, hence dot product = 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66989086-e2f8-4d1b-b028-28869b204247",
   "metadata": {},
   "source": [
    "# 💻 **Dot Product in Python**\n",
    "\n",
    "We’ll compute dot products using **NumPy**, **PyTorch**, and **TensorFlow**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b64559d-e59c-4332-9a9e-a48c0d4cf81d",
   "metadata": {},
   "source": [
    "## **1. NumPy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8d60ba1-1961-4185-b0a8-3885c822ec9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Dot1 Product: 32\n",
      "NumPy Dot2 Product: 32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example vectors\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "# Dot product\n",
    "dot1 = np.dot(a, b)\n",
    "dot2 = a @ b   # alternative\n",
    "\n",
    "print(\"NumPy Dot1 Product:\", dot1)\n",
    "print(\"NumPy Dot2 Product:\", dot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0147cb-0cce-49b9-98fd-6acbeeaf8686",
   "metadata": {},
   "source": [
    "## **2. PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea49daaa-e11f-4eb8-a30b-0e3702f8fa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Dot Product: 32.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example vectors\n",
    "a_t = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "b_t = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "\n",
    "# Dot product\n",
    "dot1 = torch.dot(a_t, b_t)\n",
    "dot2 = a_t @ b_t  # alternative\n",
    "\n",
    "print(\"PyTorch Dot Product:\", dot1.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907891c-9718-4f86-aaf1-7dd82bc7a497",
   "metadata": {},
   "source": [
    "## **3. TensorFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "891ae955-b3aa-4a1f-89e8-8027ae966e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Dot1 Product: 32.0\n",
      "TensorFlow Dot2 Product: 32.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Example vectors\n",
    "a_tf = tf.constant([1, 2, 3], dtype=tf.float32)\n",
    "b_tf = tf.constant([4, 5, 6], dtype=tf.float32)\n",
    "\n",
    "# Dot product\n",
    "dot1 = tf.tensordot(a_tf, b_tf, axes=1)\n",
    "dot2 = tf.reduce_sum(a_tf * b_tf)\n",
    "\n",
    "print(\"TensorFlow Dot1 Product:\", dot1.numpy())\n",
    "print(\"TensorFlow Dot2 Product:\", dot2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc9af48-c6d7-4e50-a2e3-c29df340f509",
   "metadata": {},
   "source": [
    "# 🧮 **More Dot Product Coding Examples**\n",
    "\n",
    "We’ll cover:\n",
    "\n",
    "* ✅ 1D vector dot product\n",
    "* ✅ 2D matrix dot product\n",
    "* ✅ 3D tensor dot product\n",
    "* ✅ Orthogonal vectors case\n",
    "* ✅ Broadcasting behavior\n",
    "* ✅ Dot product across specific axes\n",
    "\n",
    "---\n",
    "\n",
    "## **1. NumPy Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "70e5d3fc-16d2-4656-b845-29e1488fc46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4c30e78b-1bb2-4a4c-9810-08722a267ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D Dot Product: 32\n"
     ]
    }
   ],
   "source": [
    "# 1. 1D Vectors\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "print(\"1D Dot Product:\", np.dot(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca5b6c69-dd08-45ba-8617-6aae2be2d35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Matrix @ Vector: [17 39]\n"
     ]
    }
   ],
   "source": [
    "# 2. 2D Matrix @ Vector\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([5, 6])\n",
    "print(\"2D Matrix @ Vector:\", A @ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e64cff0d-cbb9-4132-a704-aaf26d1fed2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Matrix @ Matrix:\n",
      " [[25 28]\n",
      " [57 64]]\n"
     ]
    }
   ],
   "source": [
    "# 3. 2D Matrix @ Matrix\n",
    "B = np.array([[7, 8], [9, 10]])\n",
    "print(\"2D Matrix @ Matrix:\\n\", A @ B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50b265db-cd0a-4794-87ba-1e886f525a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal: 0\n"
     ]
    }
   ],
   "source": [
    "# 4. Orthogonal Vectors\n",
    "a = np.array([1, 0])\n",
    "b = np.array([0, 1])\n",
    "print(\"Orthogonal:\", np.dot(a, b)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fdd8872f-00f0-41eb-a639-d1caddc4d135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D Tensor Dot (axes=2):\n",
      " [[[[44 38 30]\n",
      "   [38 27 16]]\n",
      "\n",
      "  [[38 36 26]\n",
      "   [36 22 17]]\n",
      "\n",
      "  [[38 30 25]\n",
      "   [30 24 12]]]\n",
      "\n",
      "\n",
      " [[[20 18 12]\n",
      "   [18 11 10]]\n",
      "\n",
      "  [[32 26 24]\n",
      "   [26 22 13]]\n",
      "\n",
      "  [[32 30 18]\n",
      "   [30 16 13]]]]\n"
     ]
    }
   ],
   "source": [
    "# 5. 3D Tensor Dot Along Last Axis\n",
    "X = np.random.randint(1, 5, (2, 3, 4))\n",
    "Y = np.random.randint(1, 5, (2, 3, 4))\n",
    "print(\"3D Tensor Dot (axes=2):\\n\", np.tensordot(X, Y, axes=([2],[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f3255-bc7a-4056-aff4-8664f568e798",
   "metadata": {},
   "source": [
    "## **2. PyTorch Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f045f777-3cc7-47f8-9dc6-674002ec3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c341038a-e4dd-4073-bb01-476a7652ad6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D Dot Product: 32.0\n"
     ]
    }
   ],
   "source": [
    "# 6. 1D Vectors\n",
    "a = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "b = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "print(\"1D Dot Product:\", torch.dot(a, b).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "075e7fc2-5e14-4b7a-ae46-b29c5328526c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Matrix @ Vector: [17.0, 39.0]\n"
     ]
    }
   ],
   "source": [
    "# 7. 2D Matrix @ Vector\n",
    "A = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "b = torch.tensor([5, 6], dtype=torch.float32)\n",
    "print(\"2D Matrix @ Vector:\", (A @ b).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c413e080-46c9-4e95-a1a4-d05e7e58cd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Matrix @ Matrix:\n",
      " tensor([[25., 28.],\n",
      "        [57., 64.]])\n"
     ]
    }
   ],
   "source": [
    "# 8. 2D Matrix @ Matrix\n",
    "B = torch.tensor([[7, 8], [9, 10]], dtype=torch.float32)\n",
    "print(\"2D Matrix @ Matrix:\\n\", (A @ B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1b661018-b14b-49f4-87af-4892de85d7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 9. Orthogonal Vectors\n",
    "a = torch.tensor([1, 0], dtype=torch.float32)\n",
    "b = torch.tensor([0, 1], dtype=torch.float32)\n",
    "print(\"Orthogonal:\", torch.dot(a, b).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b9d86f24-6059-48e8-b0c7-c6b81db449a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D Tensor Dot (einsum):\n",
      " tensor([[36., 19., 30.],\n",
      "        [21., 34., 15.]])\n"
     ]
    }
   ],
   "source": [
    "# 10. 3D Tensor Dot\n",
    "X = torch.randint(1, 5, (2, 3, 4), dtype=torch.float32)\n",
    "Y = torch.randint(1, 5, (2, 3, 4), dtype=torch.float32)\n",
    "print(\"3D Tensor Dot (einsum):\\n\", torch.einsum(\"ijk,ijk->ij\", X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34655b69-42cd-474d-ab23-c2c8a98667d2",
   "metadata": {},
   "source": [
    "## **3. TensorFlow Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "578e2865-33ba-4761-980a-d128a41956a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4b2ed8a8-6fea-4540-a2a2-728ecc3a5e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D Dot Product: 32.0\n"
     ]
    }
   ],
   "source": [
    "# 11. 1D Vectors\n",
    "a = tf.constant([1, 2, 3], dtype=tf.float32)\n",
    "b = tf.constant([4, 5, 6], dtype=tf.float32)\n",
    "print(\"1D Dot Product:\", tf.tensordot(a, b, axes=1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "838f678d-18c1-434d-b5b4-2ea09895d0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Matrix @ Vector: [17. 39.]\n"
     ]
    }
   ],
   "source": [
    "# 12. 2D Matrix @ Vector\n",
    "A = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "b = tf.constant([5, 6], dtype=tf.float32)\n",
    "print(\"2D Matrix @ Vector:\", tf.linalg.matvec(A, b).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2e4d5023-8d54-47d1-8425-3d0e385aa8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Matrix @ Matrix:\n",
      " [[25. 28.]\n",
      " [57. 64.]]\n"
     ]
    }
   ],
   "source": [
    "# 13. 2D Matrix @ Matrix\n",
    "B = tf.constant([[7, 8], [9, 10]], dtype=tf.float32)\n",
    "print(\"2D Matrix @ Matrix:\\n\", tf.matmul(A, B).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cfdcea50-0fce-4baa-a7aa-7874c280c34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 14. Orthogonal Vectors\n",
    "a = tf.constant([1, 0], dtype=tf.float32)\n",
    "b = tf.constant([0, 1], dtype=tf.float32)\n",
    "print(\"Orthogonal:\", tf.tensordot(a, b, axes=1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ce43ef2-9d3e-479f-9a08-19c56bbb9809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D Tensor Dot (axes=2):\n",
      " [[[[23 33 36]\n",
      "   [29 23 29]]\n",
      "\n",
      "  [[23 33 39]\n",
      "   [32 25 29]]\n",
      "\n",
      "  [[14 19 22]\n",
      "   [17 11 14]]]\n",
      "\n",
      "\n",
      " [[[17 22 26]\n",
      "   [20 12 15]]\n",
      "\n",
      "  [[12 19 27]\n",
      "   [24 21 20]]\n",
      "\n",
      "  [[16 24 26]\n",
      "   [22 20 24]]]]\n"
     ]
    }
   ],
   "source": [
    "# 15. 3D Tensor Dot\n",
    "X = tf.random.uniform((2, 3, 4), minval=1, maxval=5, dtype=tf.int32)\n",
    "Y = tf.random.uniform((2, 3, 4), minval=1, maxval=5, dtype=tf.int32)\n",
    "print(\"3D Tensor Dot (axes=2):\\n\", tf.tensordot(X, Y, axes=[[2], [2]]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02693340-0aa4-4d4c-b462-c2aa9577ce07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc1067c-bb7b-4b7a-884c-8687fe837ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b4c2df-7082-4d90-8600-7c09b710e7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
