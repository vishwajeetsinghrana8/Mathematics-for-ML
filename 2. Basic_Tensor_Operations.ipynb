{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ae7235-ab5b-4e5b-a077-5f38ccf4c416",
   "metadata": {},
   "source": [
    "## Section-2: Tensor Operations Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db447ce9-a713-4f16-b325-fac25f380f39",
   "metadata": {},
   "source": [
    "### **Tensor Transposition**\n",
    "\n",
    "**Tensor Transposition** is the process of rearranging (or permuting) the axes/dimensions of a tensor.\n",
    "\n",
    "Itâ€™s the higher-dimensional generalization of **matrix transposition**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Matrix Transposition (2D case)**\n",
    "\n",
    "* A **matrix** is a 2D tensor with shape `(rows, columns)`.\n",
    "* Transposing swaps its two dimensions:\n",
    "\n",
    "  $$\n",
    "  A^T_{ij} = A_{ji}\n",
    "  $$\n",
    "\n",
    "  If $A$ has shape $(m, n)$, then $A^T$ has shape $(n, m)$.\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6\n",
    "\\end{bmatrix}^T\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 4 \\\\\n",
    "2 & 5 \\\\\n",
    "3 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Tensor Transposition (n-D case)**\n",
    "\n",
    "* A **tensor** can have more than 2 dimensions, e.g., 3D: `(depth, height, width)` or `(channels, height, width)`.\n",
    "* **Tensor transposition** means **reordering the dimensions** in a specified way.\n",
    "* In notation, this is often called **permutation of axes**.\n",
    "\n",
    "If $T$ has shape `(Dâ‚, Dâ‚‚, Dâ‚ƒ)`, then:\n",
    "\n",
    "* A transpose with order `(1, 0, 2)` swaps the first two axes.\n",
    "* A transpose with order `(2, 0, 1)` moves the last axis to the front.\n",
    "\n",
    "Mathematically, for a tensor $A$ with components $A_{i_1, i_2, \\dots, i_n}$:\n",
    "\n",
    "$$\n",
    "\\text{transpose}(A, \\text{perm})_{j_1, j_2, \\dots, j_n} = A_{i_{\\text{perm}(1)}, i_{\\text{perm}(2)}, \\dots, i_{\\text{perm}(n)}}\n",
    "$$\n",
    "\n",
    "where `perm` is the permutation order.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Example**\n",
    "\n",
    "Suppose $T$ has shape `(2, 3, 4)`:\n",
    "\n",
    "* Original axes: `(0, 1, 2)` â†’ shape `(2, 3, 4)`\n",
    "* After `transpose(1, 0, 2)` â†’ shape `(3, 2, 4)`\n",
    "* After `transpose(2, 1, 0)` â†’ shape `(4, 3, 2)`\n",
    "\n",
    "In **NumPy**:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "T = np.random.randint(0, 10, (2, 3, 4))\n",
    "T1 = np.transpose(T, (1, 0, 2))  # swap axes 0 and 1\n",
    "print(T.shape)   # (2, 3, 4)\n",
    "print(T1.shape)  # (3, 2, 4)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Why Tensor Transposition is Important**\n",
    "\n",
    "* **Deep Learning**: Changing data layout for operations (e.g., `channels_first` â†” `channels_last` in CNNs).\n",
    "* **Linear Algebra**: Converting between row-major and column-major forms.\n",
    "* **Data Manipulation**: Preparing tensors for broadcasting, concatenation, or reshaping.\n",
    "* **Performance Optimization**: Some hardware prefers specific memory layouts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe8fc7-a1bd-4d90-ae15-f9e36d7920bc",
   "metadata": {},
   "source": [
    "### 1. Matrix Transposition (2D Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e288469-9800-41ca-8e46-60724a329655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2aac91-5b59-4f3b-a72b-24fdc39af46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (NumPy):\n",
      " [[1 2 3]\n",
      " [4 5 6]]\n",
      "Transpose (NumPy):\n",
      " [[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(\"Original (NumPy):\\n\", A)\n",
    "print(\"Transpose (NumPy):\\n\", A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0819cd89-beb1-40b6-8004-6378ca15363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "917678a0-1e3c-4cc7-aeae-954737d11ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (PyTorch):\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Transpose (PyTorch):\n",
      " tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(\"Original (PyTorch):\\n\", B)\n",
    "print(\"Transpose (PyTorch):\\n\", B.T)   # or torch.transpose(B, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f165d-390c-4cbf-a355-e2fbeca52ac0",
   "metadata": {},
   "source": [
    "### 2. Swap Axes in a 3D Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97df27f2-4329-429e-b01a-763e3e3a3bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5 7 4 6]\n",
      "  [7 3 0 1]\n",
      "  [5 9 3 6]]\n",
      "\n",
      " [[2 0 7 7]\n",
      "  [1 0 4 6]\n",
      "  [8 8 7 1]]]\n",
      "Shape before: (2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "T = np.random.randint(0, 10, (2, 3, 4))\n",
    "print(T)\n",
    "print(\"Shape before:\", T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f932102d-d16f-4eb7-a691-486772324508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5 7 4 6]\n",
      "  [2 0 7 7]]\n",
      "\n",
      " [[7 3 0 1]\n",
      "  [1 0 4 6]]\n",
      "\n",
      " [[5 9 3 6]\n",
      "  [8 8 7 1]]]\n",
      "Shape after (NumPy): (3, 2, 4)\n"
     ]
    }
   ],
   "source": [
    "T1 = np.transpose(T, (1, 0, 2))  # swap axis 0 and 1\n",
    "print(T1)\n",
    "print(\"Shape after (NumPy):\", T1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d5b0325-8ab0-4d20-af41-85baa5306f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[4, 0, 1, 6],\n",
      "         [4, 7, 5, 5],\n",
      "         [8, 2, 7, 9]],\n",
      "\n",
      "        [[1, 0, 3, 4],\n",
      "         [3, 6, 3, 7],\n",
      "         [5, 0, 3, 2]]])\n",
      "Shape before: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "T_torch = torch.randint(0, 10, (2, 3, 4))\n",
    "print(T_torch)\n",
    "print(\"Shape before:\", T_torch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06d2b5ac-2b35-43f8-a4d2-f87b4f8fad8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[4, 0, 1, 6],\n",
      "         [1, 0, 3, 4]],\n",
      "\n",
      "        [[4, 7, 5, 5],\n",
      "         [3, 6, 3, 7]],\n",
      "\n",
      "        [[8, 2, 7, 9],\n",
      "         [5, 0, 3, 2]]])\n",
      "Shape after (PyTorch): torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "T1_torch = T_torch.permute(1, 0, 2)  # same operation\n",
    "print(T1_torch)\n",
    "print(\"Shape after (PyTorch):\", T1_torch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc6315-fa1b-461b-8086-462412c67a98",
   "metadata": {},
   "source": [
    "### 3. Moving Last Axis to First (Channel Conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb3d5bc-e5f9-4f9b-9571-13f9a6190c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy shape: (32, 32, 3) -> (3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "img = np.random.rand(32, 32, 3)   # HWC (Height, Width, Channels)\n",
    "chw_img = np.transpose(img, (2, 0, 1))  # to CHW\n",
    "print(\"NumPy shape:\", img.shape, \"->\", chw_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "becf824f-04c5-4e02-b2fd-ebec0aaef0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch shape: torch.Size([32, 32, 3]) -> torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "img_torch = torch.rand(32, 32, 3)  # HWC\n",
    "chw_img_torch = img_torch.permute(2, 0, 1)  # to CHW\n",
    "print(\"PyTorch shape:\", img_torch.shape, \"->\", chw_img_torch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eea1d2-ebd6-41db-ae5e-db97528abd2b",
   "metadata": {},
   "source": [
    "### 4. Batch of Matrices Transpose (4D Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de901aca-0080-45b4-bcd2-e62cc154194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy shape: (10, 28, 28, 1) -> (10, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "batch = np.random.rand(10, 28, 28, 1)  # (batch, H, W, C)\n",
    "batch_T = np.transpose(batch, (0, 3, 1, 2))  # (batch, C, H, W)\n",
    "print(\"NumPy shape:\", batch.shape, \"->\", batch_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a011402-1a99-495f-88ef-9f071ec8d5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch shape: torch.Size([10, 28, 28, 1]) -> torch.Size([10, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "batch_torch = torch.rand(10, 28, 28, 1)\n",
    "batch_T_torch = batch_torch.permute(0, 3, 1, 2)\n",
    "print(\"PyTorch shape:\", batch_torch.shape, \"->\", batch_T_torch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7049c4-8bfa-46fb-b8eb-294b5c0a5443",
   "metadata": {},
   "source": [
    "### 5. Swapping Two Specific Dimensions in a Higher Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af32d25e-c7d9-4ed4-ac12-dab327a0d1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy shape: (4, 5, 6, 7) -> (4, 6, 5, 7)\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "X = np.random.rand(4, 5, 6, 7)\n",
    "X_swap = np.swapaxes(X, 1, 2)  # swap axis-1 and axis-2\n",
    "print(\"NumPy shape:\", X.shape, \"->\", X_swap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edd81743-0244-497a-b7db-bdf18aead26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch shape: torch.Size([4, 5, 6, 7]) -> torch.Size([4, 6, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "X_torch = torch.rand(4, 5, 6, 7)\n",
    "X_swap_torch = torch.transpose(X_torch, 1, 2)  # swap dim-1 and dim-2\n",
    "print(\"PyTorch shape:\", X_torch.shape, \"->\", X_swap_torch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8c8048-c3dc-4ee2-a9f8-ef4c474786f6",
   "metadata": {},
   "source": [
    "# **Introduction to Basic Tensor Arithmetical Properties**\n",
    "\n",
    "Tensors are the fundamental data structures used in modern scientific computing, machine learning, and deep learning. A tensor can be thought of as a generalized form of scalars, vectors, and matrices that can extend to any number of dimensions. Since they generalize familiar mathematical objects, tensors naturally inherit a rich set of **arithmetical properties**.\n",
    "\n",
    "Just as numbers, vectors, and matrices follow certain arithmetic rulesâ€”such as commutativity of addition, distributivity of multiplication, and the existence of identity elementsâ€”tensors also obey these properties. Understanding these rules is important because they guarantee consistency when performing operations like tensor addition, scalar multiplication, elementwise multiplication, and transposition. These properties form the **building blocks** for more advanced concepts in linear algebra, numerical computation, and deep learning frameworks like NumPy and PyTorch.\n",
    "\n",
    "At the most basic level, tensor arithmetic is performed **elementwise** (for addition and multiplication) or **structurally** (for operations like transpose). For example, when two tensors of the same shape are added, the operation applies entry by entry. Similarly, transposing a tensor rearranges its axes, but still preserves key algebraic properties such as $(A + B)^T = A^T + B^T$.\n",
    "\n",
    "By studying these properties, we gain a solid foundation that allows us to:\n",
    "\n",
    "* Manipulate tensors confidently in code (NumPy, PyTorch).\n",
    "* Simplify mathematical expressions in linear algebra.\n",
    "* Ensure correctness when designing deep learning models.\n",
    "* Build intuition for higher-level operations like matrix multiplication, broadcasting, and tensor contractions.\n",
    "\n",
    "In the following sections, we will explore the most important **basic tensor arithmetical properties**â€”including addition, scalar multiplication, elementwise multiplication, and transpose rulesâ€”using simple **2D tensors (matrices)** in NumPy and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79057ccb-7771-4e16-a155-a7802ca2fa2c",
   "metadata": {},
   "source": [
    "## âœ… **Agenda:**\n",
    "\n",
    "1. Commutativity of addition\n",
    "2. Associativity of addition\n",
    "3. Additive identity (zero tensor)\n",
    "4. Additive inverse\n",
    "5. Scalar identity (1)\n",
    "6. Scalar zero (0)\n",
    "7. Associativity of scalar multiplication\n",
    "8. Distributivity of scalar multiplication over addition\n",
    "9. Commutativity of elementwise multiplication\n",
    "10. Distributivity of elementwise multiplication\n",
    "11. Transpose of transpose\n",
    "12. Transpose of sum\n",
    "13. Transpose of scalar multiplication\n",
    "14. Transpose of elementwise product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898baf5-b4f7-4f7a-a554-c284624f62f3",
   "metadata": {},
   "source": [
    "# ðŸ“˜ **Basic Tensor Arithmetical Properties**\n",
    "\n",
    "Letâ€™s start with **example tensors**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f726c4a0-d780-4e52-9988-5bd94ab93cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy\n",
    "import numpy as np\n",
    "A = np.arange(1, 13).reshape(3, 4)  # 3x4 tensor\n",
    "B = np.ones((3, 4), dtype=int) * 2\n",
    "C = np.full((3, 4), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcf51607-4489-4460-bb20-9aebda2eec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "A_t = torch.arange(1, 13).reshape(3, 4)\n",
    "B_t = torch.ones((3, 4), dtype=torch.int32) * 2\n",
    "C_t = torch.full((3, 4), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ed7ee0c-f7f5-4158-ac3d-7d2fca2d3462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "[[2 2 2 2]\n",
      " [2 2 2 2]\n",
      " [2 2 2 2]]\n",
      "[[3 3 3 3]\n",
      " [3 3 3 3]\n",
      " [3 3 3 3]]\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "tensor([[2, 2, 2, 2],\n",
      "        [2, 2, 2, 2],\n",
      "        [2, 2, 2, 2]], dtype=torch.int32)\n",
      "tensor([[3, 3, 3, 3],\n",
      "        [3, 3, 3, 3],\n",
      "        [3, 3, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "print(B)\n",
    "print(C)\n",
    "\n",
    "print(A_t)\n",
    "print(B_t)\n",
    "print(C_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc0152-9599-49e8-9d13-2ca0cd33372a",
   "metadata": {},
   "source": [
    "So we have:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "9 & 10 & 11 & 12\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "2 & 2 & 2 & 2 \\\\\n",
    "2 & 2 & 2 & 2 \\\\\n",
    "2 & 2 & 2 & 2\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "C =\n",
    "\\begin{bmatrix}\n",
    "3 & 3 & 3 & 3 \\\\\n",
    "3 & 3 & 3 & 3 \\\\\n",
    "3 & 3 & 3 & 3\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1afe1-4bf3-42f2-98e5-991add79515b",
   "metadata": {},
   "source": [
    "## **1. Addition is Commutative**\n",
    "\n",
    "$$\n",
    "A + B = B + A\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4564654f-a2ab-4578-88ff-ca1d2dfc263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 3  4  5  6]\n",
      " [ 7  8  9 10]\n",
      " [11 12 13 14]] \n",
      " [[ 3  4  5  6]\n",
      " [ 7  8  9 10]\n",
      " [11 12 13 14]]\n",
      "PyTorch:\n",
      " tensor([[ 3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10],\n",
      "        [11, 12, 13, 14]]) \n",
      " tensor([[ 3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10],\n",
      "        [11, 12, 13, 14]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", A + B, \"\\n\", B + A)\n",
    "print(\"PyTorch:\\n\", A_t + B_t, \"\\n\", B_t + A_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc787c6-bad9-44a3-92f6-5b131a58eaaa",
   "metadata": {},
   "source": [
    "## **2. Addition is Associative**\n",
    "\n",
    "$$\n",
    "(A + B) + C = A + (B + C)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c8e9864-1c2d-4a89-9f88-b4fa62285210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 6  7  8  9]\n",
      " [10 11 12 13]\n",
      " [14 15 16 17]] \n",
      " [[ 6  7  8  9]\n",
      " [10 11 12 13]\n",
      " [14 15 16 17]]\n",
      "PyTorch:\n",
      " tensor([[ 6,  7,  8,  9],\n",
      "        [10, 11, 12, 13],\n",
      "        [14, 15, 16, 17]]) \n",
      " tensor([[ 6,  7,  8,  9],\n",
      "        [10, 11, 12, 13],\n",
      "        [14, 15, 16, 17]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", (A + B) + C, \"\\n\", A + (B + C))\n",
    "print(\"PyTorch:\\n\", (A_t + B_t) + C_t, \"\\n\", A_t + (B_t + C_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ecc76-495f-40b7-84a9-b6c394278657",
   "metadata": {},
   "source": [
    "## **3. Zero Element (Additive Identity)**\n",
    "\n",
    "$$\n",
    "A + 0 = A\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d155aaf6-f131-4c5d-a99d-e2455930e1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "PyTorch:\n",
      " tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "Z = np.zeros((3, 4), dtype=int)\n",
    "print(\"NumPy:\\n\", A + Z)\n",
    "\n",
    "Z_t = torch.zeros((3, 4), dtype=torch.int32)\n",
    "print(\"PyTorch:\\n\", A_t + Z_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351e133-5f86-4a30-ab61-eb8633af8230",
   "metadata": {},
   "source": [
    "## **4. Additive Inverse**\n",
    "\n",
    "$$\n",
    "A + (-A) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e80a5001-4670-49d7-93a5-78a88c79ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "PyTorch:\n",
      " tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", A + (-A))\n",
    "print(\"PyTorch:\\n\", A_t + (-A_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46276e69-f877-42da-99c7-08da6a1e6f94",
   "metadata": {},
   "source": [
    "## **5. Scalar Multiplication Identity**\n",
    "\n",
    "$$\n",
    "1 \\cdot A = A\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad35f36c-3f35-46ac-b9e5-755aa1d98ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "PyTorch:\n",
      " tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", 1 * A)\n",
    "print(\"PyTorch:\\n\", 1 * A_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197266e8-dede-4e5a-a5ea-1a6ad3ce0c4f",
   "metadata": {},
   "source": [
    "## **6. Multiplication by Zero**\n",
    "\n",
    "$$\n",
    "0 \\cdot A = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a207849-c09d-43f9-b978-046fbe0d1f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "PyTorch:\n",
      " tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", 0 * A)\n",
    "print(\"PyTorch:\\n\", 0 * A_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68abf0ff-ea5a-4438-a820-47ddcfcf58c9",
   "metadata": {},
   "source": [
    "## **7. Scalar Multiplication is Associative**\n",
    "\n",
    "$$\n",
    "(\\alpha \\beta) A = \\alpha (\\beta A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f759e116-23cb-43e1-9487-4041c3c75fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 6 12 18 24]\n",
      " [30 36 42 48]\n",
      " [54 60 66 72]] \n",
      " [[ 6 12 18 24]\n",
      " [30 36 42 48]\n",
      " [54 60 66 72]]\n",
      "PyTorch:\n",
      " tensor([[ 6, 12, 18, 24],\n",
      "        [30, 36, 42, 48],\n",
      "        [54, 60, 66, 72]]) \n",
      " tensor([[ 6, 12, 18, 24],\n",
      "        [30, 36, 42, 48],\n",
      "        [54, 60, 66, 72]])\n"
     ]
    }
   ],
   "source": [
    "alpha, beta = 2, 3\n",
    "print(\"NumPy:\\n\", (alpha * beta) * A, \"\\n\", alpha * (beta * A))\n",
    "print(\"PyTorch:\\n\", (alpha * beta) * A_t, \"\\n\", alpha * (beta * A_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7216d7e9-2098-4b25-9e0b-f030c796de81",
   "metadata": {},
   "source": [
    "## **8. Scalar Multiplication Distributes Over Addition**\n",
    "\n",
    "$$\n",
    "\\alpha (A + B) = \\alpha A + \\alpha B\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2be02e3-9f39-41e1-9048-919a32489313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[15 20 25 30]\n",
      " [35 40 45 50]\n",
      " [55 60 65 70]] \n",
      " [[15 20 25 30]\n",
      " [35 40 45 50]\n",
      " [55 60 65 70]]\n",
      "PyTorch:\n",
      " tensor([[15, 20, 25, 30],\n",
      "        [35, 40, 45, 50],\n",
      "        [55, 60, 65, 70]]) \n",
      " tensor([[15, 20, 25, 30],\n",
      "        [35, 40, 45, 50],\n",
      "        [55, 60, 65, 70]])\n"
     ]
    }
   ],
   "source": [
    "alpha = 5\n",
    "print(\"NumPy:\\n\", alpha * (A + B), \"\\n\", alpha * A + alpha * B)\n",
    "print(\"PyTorch:\\n\", alpha * (A_t + B_t), \"\\n\", alpha * A_t + alpha * B_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb5ca8-9672-4f55-a16a-ba2251fd64b8",
   "metadata": {},
   "source": [
    "## **9. Elementwise Multiplication is Commutative**\n",
    "\n",
    "$$\n",
    "A \\odot B = B \\odot A\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d0ed347-2325-4ab1-9e28-1e2c6c94fd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 2  4  6  8]\n",
      " [10 12 14 16]\n",
      " [18 20 22 24]] \n",
      " [[ 2  4  6  8]\n",
      " [10 12 14 16]\n",
      " [18 20 22 24]]\n",
      "PyTorch:\n",
      " tensor([[ 2,  4,  6,  8],\n",
      "        [10, 12, 14, 16],\n",
      "        [18, 20, 22, 24]]) \n",
      " tensor([[ 2,  4,  6,  8],\n",
      "        [10, 12, 14, 16],\n",
      "        [18, 20, 22, 24]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", A * B, \"\\n\", B * A)\n",
    "print(\"PyTorch:\\n\", A_t * B_t, \"\\n\", B_t * A_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255acd62-7817-40d9-960f-ad6053b07aed",
   "metadata": {},
   "source": [
    "## **10. Elementwise Multiplication Distributes Over Addition**\n",
    "\n",
    "$$\n",
    "A \\odot (B + C) = A \\odot B + A \\odot C\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a08517a1-497e-42c1-a7ee-5a1cc668df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 5 10 15 20]\n",
      " [25 30 35 40]\n",
      " [45 50 55 60]] \n",
      " [[ 5 10 15 20]\n",
      " [25 30 35 40]\n",
      " [45 50 55 60]]\n",
      "PyTorch:\n",
      " tensor([[ 5, 10, 15, 20],\n",
      "        [25, 30, 35, 40],\n",
      "        [45, 50, 55, 60]]) \n",
      " tensor([[ 5, 10, 15, 20],\n",
      "        [25, 30, 35, 40],\n",
      "        [45, 50, 55, 60]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", A * (B + C), \"\\n\", A * B + A * C)\n",
    "print(\"PyTorch:\\n\", A_t * (B_t + C_t), \"\\n\", A_t * B_t + A_t * C_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab643af-786a-4c09-85be-9a1dcedea311",
   "metadata": {},
   "source": [
    "## **11. Transpose of Transpose**\n",
    "\n",
    "$$\n",
    "(A^T)^T = A\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f50594c0-4d06-48d7-9434-2a2227f9638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "PyTorch:\n",
      " tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", (A.T).T)\n",
    "print(\"PyTorch:\\n\", (A_t.T).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e59c8-901c-49c3-ba19-eef033c1b4ca",
   "metadata": {},
   "source": [
    "## **12. Transpose of Sum**\n",
    "\n",
    "$$\n",
    "(A + B)^T = A^T + B^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18f2ce9d-d07b-4422-93cd-2959bd0b0c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 3  7 11]\n",
      " [ 4  8 12]\n",
      " [ 5  9 13]\n",
      " [ 6 10 14]] \n",
      " [[ 3  7 11]\n",
      " [ 4  8 12]\n",
      " [ 5  9 13]\n",
      " [ 6 10 14]]\n",
      "PyTorch:\n",
      " tensor([[ 3,  7, 11],\n",
      "        [ 4,  8, 12],\n",
      "        [ 5,  9, 13],\n",
      "        [ 6, 10, 14]]) \n",
      " tensor([[ 3,  7, 11],\n",
      "        [ 4,  8, 12],\n",
      "        [ 5,  9, 13],\n",
      "        [ 6, 10, 14]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", (A + B).T, \"\\n\", A.T + B.T)\n",
    "print(\"PyTorch:\\n\", (A_t + B_t).T, \"\\n\", A_t.T + B_t.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb347c0-19b8-4e05-b915-065cb3eb1529",
   "metadata": {},
   "source": [
    "## **13. Transpose of Scalar Multiple**\n",
    "\n",
    "$$\n",
    "(\\alpha A)^T = \\alpha A^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1217f3d1-1b62-46bc-b0c9-f314348b1614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 4 20 36]\n",
      " [ 8 24 40]\n",
      " [12 28 44]\n",
      " [16 32 48]] \n",
      " [[ 4 20 36]\n",
      " [ 8 24 40]\n",
      " [12 28 44]\n",
      " [16 32 48]]\n",
      "PyTorch:\n",
      " tensor([[ 4, 20, 36],\n",
      "        [ 8, 24, 40],\n",
      "        [12, 28, 44],\n",
      "        [16, 32, 48]]) \n",
      " tensor([[ 4, 20, 36],\n",
      "        [ 8, 24, 40],\n",
      "        [12, 28, 44],\n",
      "        [16, 32, 48]])\n"
     ]
    }
   ],
   "source": [
    "alpha = 4\n",
    "print(\"NumPy:\\n\", (alpha * A).T, \"\\n\", alpha * (A.T))\n",
    "print(\"PyTorch:\\n\", (alpha * A_t).T, \"\\n\", alpha * (A_t.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b8798a-0e93-450b-acbe-dcbdfd1c340e",
   "metadata": {},
   "source": [
    "## **14. Transpose of Elementwise Product**\n",
    "\n",
    "$$\n",
    "(A \\odot B)^T = A^T \\odot B^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2e99f79-93fb-47ae-8663-d8df55b63016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:\n",
      " [[ 2 10 18]\n",
      " [ 4 12 20]\n",
      " [ 6 14 22]\n",
      " [ 8 16 24]] \n",
      " [[ 2 10 18]\n",
      " [ 4 12 20]\n",
      " [ 6 14 22]\n",
      " [ 8 16 24]]\n",
      "PyTorch:\n",
      " tensor([[ 2, 10, 18],\n",
      "        [ 4, 12, 20],\n",
      "        [ 6, 14, 22],\n",
      "        [ 8, 16, 24]]) \n",
      " tensor([[ 2, 10, 18],\n",
      "        [ 4, 12, 20],\n",
      "        [ 6, 14, 22],\n",
      "        [ 8, 16, 24]])\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy:\\n\", (A * B).T, \"\\n\", A.T * B.T)\n",
    "print(\"PyTorch:\\n\", (A_t * B_t).T, \"\\n\", A_t.T * B_t.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee63be3-1e8e-486a-a641-149ab28f21fa",
   "metadata": {},
   "source": [
    "# **What is Tensor Reduction?**\n",
    "\n",
    "**Tensor Reduction** refers to the process of **reducing a tensor along one or more dimensions (axes)** by applying an operation such as **sum, mean, max, min, product, etc.**\n",
    "\n",
    "Instead of keeping the full tensor, we â€œreduceâ€ its dimensionality by aggregating values.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Intuition**\n",
    "\n",
    "* Suppose we have a **2D tensor (matrix)**:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* **Full reduction (no axis specified):**\n",
    "  Sum of all elements â†’ $1+2+\\dots+9 = 45$\n",
    "  Mean of all elements â†’ $\\frac{45}{9} = 5$\n",
    "\n",
    "* **Reduction along axis 0 (rows):**\n",
    "  Collapse rows â†’ column-wise operation\n",
    "\n",
    "  $$\n",
    "  \\text{sum}(A, \\text{axis}=0) = [12, 15, 18]\n",
    "  $$\n",
    "\n",
    "* **Reduction along axis 1 (columns):**\n",
    "  Collapse columns â†’ row-wise operation\n",
    "\n",
    "  $$\n",
    "  \\text{sum}(A, \\text{axis}=1) = [6, 15, 24]\n",
    "  $$\n",
    "\n",
    "ðŸ‘‰ In short: **Reduction shrinks dimensions by aggregating values.**\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Common Reduction Operations**\n",
    "\n",
    "### Reduction Operations You Can Perform:\n",
    "\n",
    "1. **Sum** (`sum`, `reduce_sum`)\n",
    "2. **Mean** (`mean`, `reduce_mean`)\n",
    "3. **Max / Min** (`max`, `reduce_max`, `min`, `reduce_min`)\n",
    "4. **Product** (`prod`, `reduce_prod`)\n",
    "5. **Argmax / Argmin** (indices of extrema)\n",
    "6. **Std / Variance** (`std`, `var`, `reduce_std`, `reduce_variance`)\n",
    "7. **Any / All** (logical reductions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d8102d-5a35-4fd9-bb8a-2dae2e6c223b",
   "metadata": {},
   "source": [
    "## ðŸ“˜ **Example Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcbcff49-3d3c-47a6-b36e-e0b148f15bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ce5b0f9-65f1-46d9-a676-6eeffe4f100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy\n",
    "A_np = np.array([[1, 2, 3],\n",
    "                 [4, 5, 6],\n",
    "                 [7, 8, 9]])\n",
    "\n",
    "# PyTorch\n",
    "A_torch = torch.tensor([[1, 2, 3],\n",
    "                        [4, 5, 6],\n",
    "                        [7, 8, 9]])\n",
    "\n",
    "# TensorFlow\n",
    "A_tf = tf.constant([[1, 2, 3],\n",
    "                    [4, 5, 6],\n",
    "                    [7, 8, 9]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747d36c-010b-4332-8e28-44ea87275de7",
   "metadata": {},
   "source": [
    "## **1. Sum Reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22095379-6457-423b-bbd1-1a803080143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Sum (all): 45\n",
      "NumPy Sum axis=0: [12 15 18]\n",
      "NumPy Sum axis=1: [ 6 15 24]\n",
      "PyTorch Sum (all): tensor(45)\n",
      "PyTorch Sum dim=0: tensor([12, 15, 18])\n",
      "PyTorch Sum dim=1: tensor([ 6, 15, 24])\n",
      "TF Sum (all): 45\n",
      "TF Sum axis=0: [12 15 18]\n",
      "TF Sum axis=1: [ 6 15 24]\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Sum (all):\", np.sum(A_np))  \n",
    "print(\"NumPy Sum axis=0:\", np.sum(A_np, axis=0))  \n",
    "print(\"NumPy Sum axis=1:\", np.sum(A_np, axis=1))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Sum (all):\", torch.sum(A_torch))  \n",
    "print(\"PyTorch Sum dim=0:\", torch.sum(A_torch, dim=0))  \n",
    "print(\"PyTorch Sum dim=1:\", torch.sum(A_torch, dim=1))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Sum (all):\", tf.reduce_sum(A_tf).numpy())  \n",
    "print(\"TF Sum axis=0:\", tf.reduce_sum(A_tf, axis=0).numpy())  \n",
    "print(\"TF Sum axis=1:\", tf.reduce_sum(A_tf, axis=1).numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b858979-b17a-4732-9fb0-4114ae0d4957",
   "metadata": {},
   "source": [
    "## **2. Mean Reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3e82506-f7d1-4c52-ad9f-604f4774d6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Mean (all): 5.0\n",
      "NumPy Mean axis=0: [4. 5. 6.]\n",
      "NumPy Mean axis=1: [2. 5. 8.]\n",
      "PyTorch Mean (all): tensor(5.)\n",
      "PyTorch Mean dim=0: tensor([4., 5., 6.])\n",
      "PyTorch Mean dim=1: tensor([2., 5., 8.])\n",
      "TF Mean (all): 5\n",
      "TF Mean axis=0: [4 5 6]\n",
      "TF Mean axis=1: [2 5 8]\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Mean (all):\", np.mean(A_np))  \n",
    "print(\"NumPy Mean axis=0:\", np.mean(A_np, axis=0))  \n",
    "print(\"NumPy Mean axis=1:\", np.mean(A_np, axis=1))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Mean (all):\", torch.mean(A_torch.float()))  \n",
    "print(\"PyTorch Mean dim=0:\", torch.mean(A_torch.float(), dim=0))  \n",
    "print(\"PyTorch Mean dim=1:\", torch.mean(A_torch.float(), dim=1))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Mean (all):\", tf.reduce_mean(A_tf).numpy())  \n",
    "print(\"TF Mean axis=0:\", tf.reduce_mean(A_tf, axis=0).numpy())  \n",
    "print(\"TF Mean axis=1:\", tf.reduce_mean(A_tf, axis=1).numpy())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e4bb10-f600-4358-9b98-06120365acd5",
   "metadata": {},
   "source": [
    "## **3. Max Reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3764802a-84df-42f9-8abd-fcc6345320d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Max (all): 9\n",
      "NumPy Max axis=0: [7 8 9]\n",
      "NumPy Max axis=1: [3 6 9]\n",
      "PyTorch Max (all): tensor(9)\n",
      "PyTorch Max dim=0: torch.return_types.max(\n",
      "values=tensor([7, 8, 9]),\n",
      "indices=tensor([2, 2, 2]))\n",
      "PyTorch Max dim=1: torch.return_types.max(\n",
      "values=tensor([3, 6, 9]),\n",
      "indices=tensor([2, 2, 2]))\n",
      "TF Max (all): 9\n",
      "TF Max axis=0: [7 8 9]\n",
      "TF Max axis=1: [3 6 9]\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Max (all):\", np.max(A_np))  \n",
    "print(\"NumPy Max axis=0:\", np.max(A_np, axis=0))  \n",
    "print(\"NumPy Max axis=1:\", np.max(A_np, axis=1))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Max (all):\", torch.max(A_torch))  \n",
    "print(\"PyTorch Max dim=0:\", torch.max(A_torch, dim=0))  \n",
    "print(\"PyTorch Max dim=1:\", torch.max(A_torch, dim=1))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Max (all):\", tf.reduce_max(A_tf).numpy())  \n",
    "print(\"TF Max axis=0:\", tf.reduce_max(A_tf, axis=0).numpy())  \n",
    "print(\"TF Max axis=1:\", tf.reduce_max(A_tf, axis=1).numpy())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608524c3-9ecb-43fa-aa1d-830f455857a3",
   "metadata": {},
   "source": [
    "## **4. Min Reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b380018-95ee-4f54-aaea-70ef03cf4e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Min (all): 1\n",
      "NumPy Min axis=0: [1 2 3]\n",
      "NumPy Min axis=1: [1 4 7]\n",
      "PyTorch Min (all): tensor(1)\n",
      "PyTorch Min dim=0: torch.return_types.min(\n",
      "values=tensor([1, 2, 3]),\n",
      "indices=tensor([0, 0, 0]))\n",
      "PyTorch Min dim=1: torch.return_types.min(\n",
      "values=tensor([1, 4, 7]),\n",
      "indices=tensor([0, 0, 0]))\n",
      "TF Min (all): 1\n",
      "TF Min axis=0: [1 2 3]\n",
      "TF Min axis=1: [1 4 7]\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Min (all):\", np.min(A_np))  \n",
    "print(\"NumPy Min axis=0:\", np.min(A_np, axis=0))  \n",
    "print(\"NumPy Min axis=1:\", np.min(A_np, axis=1))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Min (all):\", torch.min(A_torch))  \n",
    "print(\"PyTorch Min dim=0:\", torch.min(A_torch, dim=0))  \n",
    "print(\"PyTorch Min dim=1:\", torch.min(A_torch, dim=1))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Min (all):\", tf.reduce_min(A_tf).numpy())  \n",
    "print(\"TF Min axis=0:\", tf.reduce_min(A_tf, axis=0).numpy())  \n",
    "print(\"TF Min axis=1:\", tf.reduce_min(A_tf, axis=1).numpy())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a9f7c-0acc-4f37-ac83-02d5973b8d28",
   "metadata": {},
   "source": [
    "## **5. Product Reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f09502dd-b48a-42b9-9e49-ef114bd5c45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Prod (all): 362880\n",
      "NumPy Prod axis=0: [ 28  80 162]\n",
      "NumPy Prod axis=1: [  6 120 504]\n",
      "PyTorch Prod (all): tensor(362880)\n",
      "PyTorch Prod dim=0: tensor([ 28,  80, 162])\n",
      "PyTorch Prod dim=1: tensor([  6, 120, 504])\n",
      "TF Prod (all): 362880\n",
      "TF Prod axis=0: [ 28  80 162]\n",
      "TF Prod axis=1: [  6 120 504]\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Prod (all):\", np.prod(A_np))  \n",
    "print(\"NumPy Prod axis=0:\", np.prod(A_np, axis=0))  \n",
    "print(\"NumPy Prod axis=1:\", np.prod(A_np, axis=1))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Prod (all):\", torch.prod(A_torch))  \n",
    "print(\"PyTorch Prod dim=0:\", torch.prod(A_torch, dim=0))  \n",
    "print(\"PyTorch Prod dim=1:\", torch.prod(A_torch, dim=1))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Prod (all):\", tf.reduce_prod(A_tf).numpy())  \n",
    "print(\"TF Prod axis=0:\", tf.reduce_prod(A_tf, axis=0).numpy())  \n",
    "print(\"TF Prod axis=1:\", tf.reduce_prod(A_tf, axis=1).numpy())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a483dc-ef5d-4aae-b063-901838357bd7",
   "metadata": {},
   "source": [
    "## **6. Argmax (Index of Max)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd211621-fdde-4d2f-a1d5-50b1918677d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Argmax (all): 8\n",
      "NumPy Argmax axis=0: [2 2 2]\n",
      "NumPy Argmax axis=1: [2 2 2]\n",
      "PyTorch Argmax (all): tensor(8)\n",
      "PyTorch Argmax dim=0: tensor([2, 2, 2])\n",
      "PyTorch Argmax dim=1: tensor([2, 2, 2])\n",
      "TF Argmax axis=0: [2 2 2]\n",
      "TF Argmax axis=1: [2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Argmax (all):\", np.argmax(A_np))  \n",
    "print(\"NumPy Argmax axis=0:\", np.argmax(A_np, axis=0))  \n",
    "print(\"NumPy Argmax axis=1:\", np.argmax(A_np, axis=1))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Argmax (all):\", torch.argmax(A_torch))  \n",
    "print(\"PyTorch Argmax dim=0:\", torch.argmax(A_torch, dim=0))  \n",
    "print(\"PyTorch Argmax dim=1:\", torch.argmax(A_torch, dim=1))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Argmax axis=0:\", tf.argmax(A_tf, axis=0).numpy())  \n",
    "print(\"TF Argmax axis=1:\", tf.argmax(A_tf, axis=1).numpy())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50981160-b944-4200-9ef7-f137bcc7daa8",
   "metadata": {},
   "source": [
    "## **7. Argmin (Index of Min)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "379e0771-755f-4e5d-9d78-80c7cc888fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Argmin (all): 0\n",
      "NumPy Argmin axis=0: [0 0 0]\n",
      "NumPy Argmin axis=1: [0 0 0]\n",
      "PyTorch Argmin (all): tensor(0)\n",
      "PyTorch Argmin dim=0: tensor([0, 0, 0])\n",
      "PyTorch Argmin dim=1: tensor([0, 0, 0])\n",
      "TF Argmin axis=0: [0 0 0]\n",
      "TF Argmin axis=1: [0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Argmin (all):\", np.argmin(A_np))  \n",
    "print(\"NumPy Argmin axis=0:\", np.argmin(A_np, axis=0))  \n",
    "print(\"NumPy Argmin axis=1:\", np.argmin(A_np, axis=1))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Argmin (all):\", torch.argmin(A_torch))  \n",
    "print(\"PyTorch Argmin dim=0:\", torch.argmin(A_torch, dim=0))  \n",
    "print(\"PyTorch Argmin dim=1:\", torch.argmin(A_torch, dim=1))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Argmin axis=0:\", tf.argmin(A_tf, axis=0).numpy())  \n",
    "print(\"TF Argmin axis=1:\", tf.argmin(A_tf, axis=1).numpy())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8d896-b357-4718-ada4-6022b5912637",
   "metadata": {},
   "source": [
    "## **8. Standard Deviation & Variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29f2d447-3361-456d-9fc3-02593e073489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Std (all): 2.581988897471611\n",
      "NumPy Var (all): 6.666666666666667\n",
      "PyTorch Std (all): tensor(2.7386)\n",
      "PyTorch Var (all): tensor(7.5000)\n",
      "TF Std (all): 2.5819888\n",
      "TF Var (all): 6.6666665\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Std (all):\", np.std(A_np))  \n",
    "print(\"NumPy Var (all):\", np.var(A_np))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Std (all):\", torch.std(A_torch.float()))  \n",
    "print(\"PyTorch Var (all):\", torch.var(A_torch.float()))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Std (all):\", tf.math.reduce_std(tf.cast(A_tf, tf.float32)).numpy())  \n",
    "print(\"TF Var (all):\", tf.math.reduce_variance(tf.cast(A_tf, tf.float32)).numpy())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64634882-9919-443c-bb60-818c75222b40",
   "metadata": {},
   "source": [
    "## **9. Logical Reductions (any, all)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51040e9f-0b01-4fa6-8af8-c5ca450acf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Any: True\n",
      "NumPy All: False\n",
      "PyTorch Any: tensor(True)\n",
      "PyTorch All: tensor(True)\n",
      "TF Any: True\n",
      "TF All: True\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "print(\"NumPy Any:\", np.any(A_np > 5))  \n",
    "print(\"NumPy All:\", np.all(A_np < 0))  \n",
    "\n",
    "# PyTorch\n",
    "print(\"PyTorch Any:\", torch.any(A_torch > 5))  \n",
    "print(\"PyTorch All:\", torch.all(A_torch > 0))  \n",
    "\n",
    "# TensorFlow\n",
    "print(\"TF Any:\", tf.reduce_any(A_tf > 5).numpy())  \n",
    "print(\"TF All:\", tf.reduce_all(A_tf > 0).numpy())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c030a-ac6f-4fcb-9760-a2c707062bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e283d19-9121-48e6-870b-47c6295a64fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
